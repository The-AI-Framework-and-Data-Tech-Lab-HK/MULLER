# SPDX-License-Identifier: MPL-2.0
#
# This Source Code Form is subject to the terms of the Mozilla Public
# License, v. 2.0. If a copy of the MPL was not distributed with this
# file, You can obtain one at https://mozilla.org/MPL/2.0/.
#
# This file was originally part of Hub (now Deep Lake) project: https://github.com/activeloopai/deeplake/tree/release/2.8.5
# Commit: https://github.com/activeloopai/deeplake/tree/94c5e100292c164b80132baf741ef233dd41f3d7
# Source: https://github.com/activeloopai/deeplake/blob/94c5e100292c164b80132baf741ef233dd41f3d7/hub/core/meta/encode/chunk_id.py
#
# Modifications Copyright (c) 2026 Bingyu Liu

from typing import Any, List, Tuple, Optional
from uuid import uuid4

import numpy as np

from muller.constants import CHUNK_ID_COLUMN, ENCODING_DTYPE
from muller.core.meta.encode.base_encoder import Encoder, LAST_SEEN_INDEX_COLUMN
from muller.core.serialize import serialize_chunkids, deserialize_chunkids
from muller.core.storage.muller_memory_object import MULLERMemoryObject
from muller.util.exceptions import (
    ChunkIdEncoderError,
    OutOfChunkCountError,
    OutOfSampleCountError,
)


class ChunkIdEncoder(Encoder, MULLERMemoryObject):
    def __init__(self, encoded=None, dtype=ENCODING_DTYPE):
        super().__init__(encoded, dtype)

    def __getitem__(
        self, local_sample_index: int, return_row_index: bool = False
    ) -> Any:
        """Derives the value at `local_sample_index`.
        It returns a singular derived value, or a tuple with the derived value and the row index.
        """
        if local_sample_index < 0:
            local_sample_index += self.num_samples

        start_row = self.translate_index(local_sample_index)
        output: List[Any] = []

        def _append_value_at(row: int):
            val = self._derive_value(self._encoded[row], row, local_sample_index)
            if return_row_index:
                output.append((val, row))
            else:
                output.append(val)

        # Append value at the start row
        _append_value_at(start_row)

        # Starting from the next row, we look for a group of rows that satisfy the condition
        row = start_row + 1
        max_row = len(self._encoded)
        while row < max_row:
            if self._encoded[row][LAST_SEEN_INDEX_COLUMN] != local_sample_index:
                break
            # Satisfy the condition
            self.last_row = row
            _append_value_at(row)
            row += 1

        return output

    def __setitem__(self, *args):
        raise NotImplementedError(
            "The update of ChunkIdEncoder is not implemented. "
            "There is no reason for ChunkIdEncoder to be updated now."
        )

    @property
    def encoded(self):
        """Returns whether it is encoded or not."""
        return self._encoded

    @encoded.setter
    def encoded(self, value):
        """Sets whether it is encoded or not."""
        self._encoded = value

    @property
    def num_chunks(self) -> int:
        """Returns the number of chunks stored in this ChunkIdEncoder."""
        if self.num_samples == 0:
            return 0
        return len(self._encoded)

    @staticmethod
    def name_from_id(chunk_id) -> str:
        """Returns the hex of `chunk_id` with the "0x" prefix removed. This is the chunk's name and should be used to
           determine the chunk's key.
        Can convert back into `chunk_id` using `id_from_name`. You can get the `id` for a chunk using `__getitem__`.
        """
        return hex(chunk_id)[2:]

    @staticmethod
    def id_from_name(name: str):
        """Returns the 64-bit integer from the hex `name` generated by `name_from_id`."""
        return int("0x" + name, 16)

    @staticmethod
    def _generate_id(dtype: type):
        shift = 128 - (8 * dtype(1).itemsize)
        return dtype(uuid4().int >> shift)

    @classmethod
    def frombuffer(cls, buffer: bytes):
        """Read from buffer"""
        instance = cls()
        if not buffer:
            return instance
        version, ids, dtype = deserialize_chunkids(buffer)
        if ids.nbytes:
            instance._encoded = ids
        instance.version = version
        instance.is_dirty = False
        instance.dtype = dtype
        return instance

    def tobytes(self) -> memoryview:
        """Returns a bytes representation of the ChunkIdEncoder."""
        return serialize_chunkids(self.version, self._encoded)

    def get_row_from_id(self, chunk_id: int):
        """Returns the 64-bit integer from the hex `name` generated by `name_from_id`."""
        chunk_ids = self._encoded[:, 0]
        return chunk_ids.index(chunk_id)

    def get_name_for_chunk(self, chunk_index: int) -> str:
        """
        Gets the name for the chunk at index `chunk_index`. If you need to get the name for a chunk from
           a sample index, instead use `__getitem__`, then `name_from_id`.
        """
        chunk_id = self._encoded[:, CHUNK_ID_COLUMN][chunk_index]
        return ChunkIdEncoder.name_from_id(chunk_id)

    def get_id_for_chunk(self, chunk_index: int) -> str:
        """
        Gets the if for the chunk at index `chunk_index`. If you need to get the name for a chunk from
           a sample index, instead use `__getitem__`, then `name_from_id`.
        """

        return str(self._encoded[:, CHUNK_ID_COLUMN][chunk_index])

    def get_next_chunk_id(self, row) -> Optional[str]:
        """Get the next chunk id for the given row."""
        if (
            self.num_chunks is None
            or self._encoded is None
            or not row < self.num_chunks - 1
        ):
            return None

        return str(self._encoded[row + 1][0])

    def get_prev_chunk_id(self, row) -> Optional[str]:
        """Get the previous chunk id for the given row."""
        if self.num_chunks is None or self._encoded is None or row == 0:
            return None

        return str(self._encoded[row - 1][0])

    def decrease_samples(self, row: int = 0, num_samples: int = 0):
        """Decrease sample count from encoder

        Args:
            row (int): row of the chunk
            num_samples (int): number of samples to be reduced

        Raises:
            OutOfSampleCountError: when num_samples are exceeding sample count
            OutOfChunkCountError: When the row is out of chunk bounds
        """
        if self.num_samples_at(row) < num_samples:
            raise OutOfSampleCountError()

        if self.num_chunks < row + 1:
            raise OutOfChunkCountError()

        self._encoded[row][LAST_SEEN_INDEX_COLUMN] -= num_samples

        self.is_dirty = True

    def delete_chunk_id(self, row):
        """Delete row from encoder

        Args:
            row (int): the row of chunk that needs to be deleted

        Raises:
            OutOfChunkCountError: When the row is out of chunk bounds
        """
        if row > self.num_chunks:
            raise OutOfChunkCountError

        self._encoded = np.delete(self._encoded, row, axis=0)
        self.is_dirty = True

    def update_chunk_id(self, row, chunk_id):
        """update chunk_id"""
        if row > self.num_chunks:
            raise OutOfChunkCountError
        self._encoded[row][0] = chunk_id
        self.is_dirty = True

    def generate_chunk_id(
        self, register: Optional[bool] = True, row: Optional[int] = None
    ):
        """Generates a random 64bit chunk ID using uuid4. Also prepares this ID to have samples registered to it.
        This method should be called once per chunk created.

        Args:
            register (Optional, bool): Whether the generated chunk id should be added to the encoder. Default True.
            row (Optional, int): Iterator position where the new generated id should be inserted

        Returns:
            The generated chunk id.

        Raises:
            OutOfChunkCountError: When the row is out of chunk bounds
        """
        chunk_id = self._generate_id(self.dtype)
        if register:
            if self.num_samples == 0:
                self._encoded = np.array([[chunk_id, -1]], dtype=self.dtype)

            else:
                if row is not None and row < self.num_chunks:
                    new_entry = np.array(
                        [chunk_id, self._encoded[row][LAST_SEEN_INDEX_COLUMN]]
                    )
                    self._encoded = np.insert(self._encoded, row + 1, new_entry, axis=0)
                    return chunk_id
                if row is not None and row != self.num_chunks:
                    raise OutOfChunkCountError()
                last_index = self.num_samples - 1

                new_entry = np.array(
                    [[chunk_id, last_index]],
                    dtype=self.dtype,
                )
                self._encoded = np.concatenate([self._encoded, new_entry])
        return chunk_id

    def register_samples(self, num_samples: int, row: Optional[int] = None, item: Any = None):  # type: ignore
        """Registers samples to the chunk ID that was generated last with the `generate_chunk_id` method.
        This method should be called at least once per chunk created.

        Args:
            num_samples (int): The number of samples the last chunk ID should have added to it's registration.
            row (int, Optional): The row of chunk to register the samples in.

        Raises:
            ValueError: `num_samples` should be non-negative.
            ChunkIdEncoderError: Must call `generate_chunk_id` before registering samples.
            ChunkIdEncoderError: `num_samples` can only be 0 if it is able to be a sample continuation across chunks.
        """

        super().register_samples(None, num_samples, row=row)

    def translate_index_relative_to_chunks(self, global_sample_index: int) -> int:
        """Converts `global_sample_index` into a new index that is relative to the chunk the sample belongs to.

        Example:
            Given: 2 samples in chunk 0, 2 samples in chunk 1, and 3 samples in chunk 2.
            >>> self.num_samples
            7
            >>> self.num_chunks
            3
            >>> self.translate_index_relative_to_chunks(0)
            0
            >>> self.translate_index_relative_to_chunks(1)
            1
            >>> self.translate_index_relative_to_chunks(2)
            0
            >>> self.translate_index_relative_to_chunks(3)
            1
            >>> self.translate_index_relative_to_chunks(6)
            2

        Args:
            global_sample_index (int): Index of the sample relative to the containing tensor.

        Returns:
            int: local index value between 0 and the amount of samples the chunk contains - 1.
        """

        ls = self.__getitem__(global_sample_index, return_row_index=True)  # type: ignore
        assert len(ls) == 1, len(
            ls
        )  # this method should only be called for non tiled samples
        chunk_index = ls[0][1]

        if chunk_index == 0:
            return global_sample_index

        current_entry = self._encoded[chunk_index - 1]  # type: ignore
        last_num_samples = current_entry[LAST_SEEN_INDEX_COLUMN] + 1

        return int(global_sample_index - last_num_samples)

    def pop(self, index: Optional[int] = None) -> Tuple[List, List, bool]:
        """Pops the last sample added to the encoder and returns ids of chunks to be deleted from storage.
        Returns:
            Tuple of list of affected chunk ids, their rows and boolean specifying whether
            those chunks should be deleted
        """
        if index is None:
            index = self.get_last_index_for_pop()
        out = self.__getitem__(index, return_row_index=True)  # type: ignore
        chunk_ids = [out[i][0] for i in range(len(out))]
        rows = [out[i][1] for i in range(len(out))]
        if len(chunk_ids) > 1:  # tiled sample
            self._encoded[rows[-1] + 1 :, LAST_SEEN_INDEX_COLUMN] -= 1
            self._encoded = np.delete(self._encoded, rows, axis=0)
            to_delete = True
        else:
            row = rows[0]
            prev = -1 if row == 0 else self._encoded[row - 1][LAST_SEEN_INDEX_COLUMN]
            num_samples_in_chunk = self.array[row][LAST_SEEN_INDEX_COLUMN] - prev \
                if prev != -1 else self.array[row][LAST_SEEN_INDEX_COLUMN] + np.abs(prev)

            if num_samples_in_chunk == 1:
                self._encoded = np.delete(self._encoded, row, axis=0)
                self._encoded[row:, LAST_SEEN_INDEX_COLUMN] -= 1
                to_delete = True
            elif num_samples_in_chunk > 1:
                self._encoded[row:, LAST_SEEN_INDEX_COLUMN] -= 1
                to_delete = False
            else:
                raise ValueError("No samples to pop")

        self.is_dirty = True
        return chunk_ids, rows, to_delete

    def delete_rows(self, rows: List[int]):
        """Delete rows."""
        if rows:
            for row in rows:
                prev = (
                    -1 if row == 0 else self._encoded[row - 1][LAST_SEEN_INDEX_COLUMN]
                )
                num_samples_in_chunk = (
                    self._encoded[row][LAST_SEEN_INDEX_COLUMN] - prev
                    if prev != -1
                    else self._encoded[row][LAST_SEEN_INDEX_COLUMN] + np.abs(prev)
                ).item()
                self._encoded[row:, LAST_SEEN_INDEX_COLUMN] -= int(num_samples_in_chunk)
            self._encoded = np.delete(self._encoded, rows, axis=0)
            self.is_dirty = True
            return True
        return False

    def replace_chunks_for_tiled_sample(
        self, global_sample_index: int, chunk_ids: List
    ):
        """Replace the chunks for tiled samples. """
        current_chunk_ids_and_rows = self.__getitem__(  # type: ignore
            global_sample_index, return_row_index=True
        )
        start_row = current_chunk_ids_and_rows[0][1]
        end_row = current_chunk_ids_and_rows[-1][1]
        if len(current_chunk_ids_and_rows) == len(chunk_ids):
            # inplace update
            self._encoded[start_row : end_row + 1, CHUNK_ID_COLUMN] = chunk_ids
        else:
            top = self._encoded[:start_row]
            bottom = self._encoded[end_row + 1 :]
            mid = np.empty((len(chunk_ids), 2), dtype=self.dtype)
            mid[:, CHUNK_ID_COLUMN] = chunk_ids
            mid[:, LAST_SEEN_INDEX_COLUMN] = global_sample_index
            self._encoded = np.concatenate([top, mid, bottom], axis=0)
        self.is_dirty = True

    def _validate_incoming_item(self, _, num_samples: int):
        """Raises appropriate exceptions for when `item` or `num_samples` are invalid.

        Args:
            _: General input, will be passed along to subclass methods.
            num_samples (int): Number of samples that have `item`'s value. Will be passed along to subclass methods.

        Raises:
            ValueError: For the general case, `num_samples` should be > 0.
        """
        if num_samples < 0:
            raise ValueError(f"Cannot register. The num samples {num_samples} is negative.")

        if len(self.array) == 0:
            raise ChunkIdEncoderError("Cannot register samples. There is no chunk ID.")

        if num_samples == 0 and self.num_chunks < 2:
            raise ChunkIdEncoderError(
                "Cannot register 0 num_samples "
                "(signifying a partial sample continuing the last chunk) "
                "when the last chunk does not exist."
            )

        # note: do not call super() method (num_samples can be 0)

    def _derive_next_last_index(self, last_index, num_samples: int):
        # this operation will trigger an overflow for the first addition, so suppress the warning
        np.seterr(over="ignore")
        new_last_index = last_index + self.dtype(num_samples)
        np.seterr(over="warn")

        return new_last_index

    def _combine_condition(self, *args) -> bool:
        """Always returns True because sample registration can always be done.
        Used in base encoder `register_samples`."""
        return True

    def _derive_value(self, row: np.ndarray, *_) -> np.ndarray:
        return row[CHUNK_ID_COLUMN]

    def _num_samples_in_last_chunk(self):
        return self._num_samples_in_last_row()
