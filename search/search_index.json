{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Introduction","text":"<p>At modern training scales, AI datasets are no longer curated by a single user, but collaboratively by multiple data engineers working on parallel data branches. In practice, engineers independently check out dataset branches, perform LLM-assisted data annotation and exploration, and commit their changes. As the main dataset evolves, some branches can be fast-forward merged (e.g., merging Branch 1 at t<sub>2</sub>), while others require three-way merges with conflict detection (e.g., merging Branch 2 at t<sub>3</sub>).</p> <p>However, existing data lake formats (e.g., Parquet, Lance, Iceberg, Deep Lake) do not natively support such collaborative, Git-like data workflows. To address this gap, we introduce MULLER, a novel Multimodal data lake format designed for collaborative AI data workflows, with the following key features:</p> <ul> <li>Mutimodal data support with than 12 data types of different modalities, including scalars, vectors, text, images, videos, and audio, with 20+ compression formats (e.g., LZ4, JPG, PNG, MP3, MP4, AVI, WAV).</li> <li>Data sampling, exploration, and analysis through low-latency random access and fast scan.</li> <li>Array-oriented hybrid search engine that jointly queries vector, text, and scalar data.</li> <li>Git-like data versioning with support for commit, checkout, diff, conflict detection and resolution, as well as merge. Specifically, to the best of our knowledge, MULLER is the first data lake format to support fine-grained row-level updates and three-way merges across multiple coexisting data branches.</li> <li>Seamless integration with LLM/MLLM data training and processing pipelines.</li> </ul> <p>Here is a video demo of MULLER to demonstrate the basic functions.</p>"},{"location":"getting_started/1_read_muller_object/","title":"Read as MULLER object","text":""},{"location":"getting_started/1_read_muller_object/#1-loading-multimodal-files-into-muller-objects","title":"1. Loading Multimodal Files into MULLER Objects","text":"<p>MULLER provides the <code>muller.read()</code> interface for loading files from different storage backends via the <code>path</code> argument. Below, we use image loading as an example.</p> <p>Example 1: Loading from a Local File Path <pre><code>&gt;&gt;&gt; img_1 = muller.read(path=\"/your/data/path/xxx.jpg\")\n&gt;&gt;&gt; img_1.array  # Access the tensor representation of the multimodal object via `.array`.\narray([[[183, 160, 118],\n        [223, 201, 160],\n        [211, 189, 150],\n        ...,\n        [143, 146,  79],\n        [186, 189, 122],\n        [142, 145,  78]]], shape=(400, 500, 3), dtype=uint8)\n\n&gt;&gt;&gt; img_1.shape  # Retrieve the tensor shape via the `.shape` attribute.\n(400, 500, 3)  # This JPEG image has a resolution of 400\u00d7500 (height \u00d7 width) with three RGB channels.\n\n&gt;&gt;&gt; img_1.dtype  # Retrieve the underlying data type via the `.dtype` attribute.\n'uint8'  # JPEG images represent pixel values in the range [0, 255], and are therefore stored as uint8.\n</code></pre></p> <p>Example 2: Loading from an HTTP/HTTPS URL (If access is restricted, the <code>creds</code> argument can be used to provide proxy settings.) <pre><code>&gt;&gt;&gt; credential_2 = {\n...     \"proxies\": {\n...         \"http\": \"http://account:pwd@proxy.xxx.com:port\",\n...         \"https\": \"http://account:pwd@proxy.xxx.com:port\"\n...     }\n... }\n&gt;&gt;&gt; img_2 = muller.read(\n...     path=\"https://xxx.com/xxx.jpg\",\n...     creds=credential_2\n... )\n</code></pre></p> <p>Example 3: Loading from a ROMA/S3 object storage bucket <pre><code>&gt;&gt;&gt; credential_3 = {\n...     \"bucket_name\": \"bucket-name\",\n...     \"region\": \"xxx\",\n...     \"app_token\": \"xxx\",\n...     \"vendor\": \"xxx\"\n... }\n&gt;&gt;&gt; img_3 = muller.read(\n...     path=\"roma://your/data/path/xxx.jpg\",\n...     creds=credential_3\n... )\n</code></pre></p> <ul> <li><code>muller.read()</code> currently supports multiple multimodal file types, including images, audio, and video, covering more than 20 file formats.</li> <li>The optional <code>creds</code> argument can be used to access remote or cloud storage backends (e.g., HTTP/HTTPS, ROMA, OBS), and may include authentication or connection information such as <code>proxies</code>, <code>bucket_name</code>, <code>access_key</code>, <code>secret_key</code>, and <code>token</code>.</li> <li>For the complete list of input parameters, supported file types, and usage details of <code>muller.read()</code>, please refer to the API documentation.</li> </ul>"},{"location":"getting_started/2_create_muller_dataset/","title":"Creating a MULLER Dataset","text":""},{"location":"getting_started/2_create_muller_dataset/#1-manually-loading-data-and-creating-a-muller-dataset","title":"1. Manually Loading Data and Creating a MULLER Dataset","text":""},{"location":"getting_started/2_create_muller_dataset/#step-1-create-an-empty-muller-dataset","title":"Step 1. Create an Empty MULLER Dataset","text":"<p>A MULLER dataset is represented as a directory, whose name may consist of letters, digits, underscores (<code>_</code>), and hyphens (<code>-</code>). Assume we would like to create a dataset named <code>muller_dataset</code>. An empty MULLER dataset can be created on different storage backends via the <code>path</code> argument, as shown below.</p> <p>Example 1: Local Storage (The MULLER dataset is stored on the local filesystem.)</p> <p>No prefix is required for local paths. Both relative and absolute paths are supported.</p> <p>Note: This approach is not recommended in non-local environments. File I/O operations rely directly on native libraries such as <code>os</code>, <code>pathlib</code>, and <code>shutil</code>, which may lead to lower performance.</p> <p><pre><code>&gt;&gt;&gt; ds_1 = muller.empty(path=\"muller_dataset/\")\n&gt;&gt;&gt; ds_2 = muller.empty(path=\"./my_data/muller_dataset/\")\n</code></pre> Example 2: S3 Object Storage (The MULLER dataset is stored in an S3-compatible service.)</p> <p>Use the <code>s3://</code> prefix to specify the S3 path. Credentials must be provided.</p> <p>Note: File I/O operations in this mode are implemented via the S3 client interface using boto3, which generally provides higher performance. For non-local storage backends (e.g., S3), I/O performance is highly dependent on available network bandwidth. In general, private cloud environments offer lower latency and higher bandwidth due to dedicated compute, storage, and network resources, resulting in performance close to local disk access. In contrast, bandwidth contention may significantly impact I/O throughput in shared environments. Under bandwidth constraints, MULLER workloads may become I/O-bound. It is therefore strongly recommended to reserve sufficient network bandwidth for MULLER operations.</p> <pre><code>&gt;&gt;&gt; endpoint = \"http://x.x.x.x:xxxx\"  # Example only; use a valid endpoint\n&gt;&gt;&gt; ak = \"xxx\"                              # Example only; use a valid access key\n&gt;&gt;&gt; sk = \"yyy\"                              # Example only; use a valid secret key\n&gt;&gt;&gt; bucket_name = \"zzz\"              # Example only; use a valid bucket name\n&gt;&gt;&gt; creds = {\"bucket_name\": bucket_name, \"endpoint\": endpoint, \"ak\": ak, \"sk\": sk}\n&gt;&gt;&gt; ds_3 = muller.empty(path=\"s3://muller_dataset\", creds=creds)\n</code></pre> <p>In addition to <code>muller.empty()</code>, you may also use <code>muller.dataset()</code> to create a dataset. The usage is similar to the examples above; simply add the appropriate path prefix according to the storage backend.</p> <p>Example 3: Local Storage <pre><code>&gt;&gt;&gt; muller.dataset(path=\"./my_data/muller_dataset/\")\n</code></pre></p> <p>Example 4: S3 Object Storage <pre><code>&gt;&gt;&gt; muller.dataset(path=\"s3://muller_dataset\", creds=creds)\n</code></pre> * If the path specified by <code>path</code> already contains an existing MULLER dataset, a <code>DatasetHandlerError</code> will be raised. You may set <code>overwrite=True</code> to explicitly overwrite the existing dataset. * For additional optional arguments and advanced usage of these APIs, please refer to the API documentation.</p>"},{"location":"getting_started/2_create_muller_dataset/#step-2-create-tensor-columns-and-specify-column-types","title":"Step 2. Create Tensor Columns and Specify Column Types","text":"<p>In a MULLER dataset, columns are referred to as tensor columns. MULLER adopts a column-oriented storage layout, where each tensor column can be configured with a specific column type (<code>htype</code>), sample compression format (<code>sample_compression</code>), and data type (<code>dtype</code>).</p> <p>For most column types, specifying an appropriate compression format can significantly reduce storage footprint and improve read performance.</p> <p><pre><code>&gt;&gt;&gt; ds = muller.dataset(path='my_muller_dataset/', overwrite=True)  # Create a new MULLER dataset\n&gt;&gt;&gt; ds.create_tensor(name='my_text', htype='text')\n&gt;&gt;&gt; ds.create_tensor(name='my_photos', htype='image', sample_compression='jpg')\n&gt;&gt;&gt; ds.summary()  # Inspect the dataset schema\n  tensor      htype     shape     dtype   compression\n  -------    -------   -------   -------  -----------\n  my_text     text      (0,)      str      None\n  my_photos   image     (0,)      uint8    jpeg\n</code></pre> For the <code>generic</code> column type, the data type (<code>dtype</code>) can be explicitly specified (e.g., <code>int16</code>, <code>int32</code>, <code>float32</code>, <code>bytes</code>, <code>bool</code>). For other column types, the <code>dtype</code> is predefined and does not need to be specified.</p> <p><pre><code>&gt;&gt;&gt; ds.create_tensor(name='my_label', htype='generic', dtype='int32')\n&gt;&gt;&gt; ds.summary()\n  tensor      htype     shape     dtype   compression\n  -------    -------   -------   -------  -----------\n  my_text     text      (0,)      str      None\n  my_photos   image     (0,)      uint8    jpeg\n  my_label    generic   (0,)      int32    None\n</code></pre> Currently, MULLER supports the following tensor column types (<code>htype</code>), compression formats (<code>sample_compression</code>), and default data types (<code>dtype</code>).</p> htype sample_compression dtype image Required (one of): bmp, dib, gif, ico, jpg, jpeg, jpeg2000, pcx, png, ppm, sgi, tga, tiff, webp, wmf, xbm, eps, fli, im, msp, mpo Default: <code>uint8</code> (modification not recommended) video Required (one of): mp4, mkv, avi Default: <code>uint8</code> (modification not recommended) audio Required (one of): flac, mp3, wav Default: <code>float64</code> (modification not recommended) class_label Default: None (null); Optional: lz4 Default: <code>uint32</code> (modification not recommended) bbox Default: None (null); Optional: lz4 Default: <code>float32</code> (modification not recommended) text Default: None (null); Optional: lz4 Default: <code>str</code> (modification not recommended) json Default: None (null); Optional: lz4 - list Default: None (null); Optional: lz4 - vector Default: None (null); Optional: lz4 Default:  <code>float32</code> generic Default: None (null); Optional: lz4 Default: None (undeclared, inferred from data); Declaration at creation is recommended.Options: <code>int8</code>, <code>int16</code>, <code>int32</code>, <code>int64</code>, <code>uint8</code>, <code>uint16</code>, <code>uint32</code>, <code>float32</code>, <code>float64</code>, <code>bool</code> <ul> <li>Note 1: If <code>htype</code> is not specified when creating a tensor column, it defaults to <code>generic</code>.</li> <li>Note 2: For <code>class_label</code>, <code>bbox</code>, <code>text</code>, <code>json</code>, <code>list</code>, and <code>generic</code> columns, it is recommended to leave <code>sample_compression</code> as <code>None</code> unless storage savings are critical. Using <code>lz4</code> introduces additional compression and decompression overhead, which may negatively impact read and write performance.</li> <li>Note 3: In addition to <code>htype</code>, <code>sample_compression</code>, and <code>dtype</code>, additional parameters can be specified when creating tensor columns for advanced use cases.   For example, <code>chunk_compression</code> can be configured (see chunk_compression), and <code>bbox</code> columns may accept extra parameters such as <code>coords</code> (see BoundingBox Htype).   For the complete list of supported column types and parameters, please refer to Htypes and its subpages.</li> <li>Note 4: Tensor column names must not contain any MULLER reserved keywords or any Python reserved keywords.   <pre><code>keyword_list_1 = ['__bool__', '__class__', '__del__', '__delattr__', '__dict__', '__dir__', '__doc__', '__enter__', '__eq__', '__exit__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setitem__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_all_tensors_filtered', '_append_or_extend', '_append_to_queries_json', '_check_values', '_checkout', '_checkout_hooks', '_client', '_commit', '_commit_hooks', '_copy', '_create_downsampled_tensor', '_create_sample_id_tensor', '_create_sample_info_tensor', '_create_sample_shape_tensor', '_create_tensor', '_dataset_diff', '_delete_branch', '_delete_tensor', '_deserialize_uuids', '_disable_padding', '_ds_diff', '_enable_padding', '_find_subtree', '_first_load_init', '_flush_vc_info', '_get_chunk_names', '_get_commit_id_for_address', '_get_empty_vds', '_get_inverted_index', '_get_tensor_from_root', '_get_tensor_uuids', '_get_view', '_get_view_info', '_groups', '_groups_filtered', '_has_group_in_root', '_indexing_history', '_info', '_initial_autoflush', '_inverted_index', '_is_filtered_view', '_is_root', '_link_tensors', '_load_link_creds', '_load_version_info', '_lock', '_lock_lost_handler', '_lock_queries_json', '_lock_timeout', '_locked_out', '_locking_enabled', '_pad_tensors', '_parent_dataset', '_pop', '_populate_meta', '_query_string', '_read_from_upper_cache', '_read_only', '_read_only_error', '_read_queries_json', '_register_dataset', '_reload_version_state', '_resolve_tensor_list', '_sample_indices', '_save_view', '_save_view_in_path', '_save_view_in_subdir', '_send_branch_creation_event', '_send_branch_deletion_event', '_send_commit_event', '_send_compute_progress', '_send_query_progress', '_set_derived_attributes', '_set_read_only', '_sub_ds', '_subtree_to_dict', '_temp_tensors', '_tensors', '_token', '_ungrouped_tensors', '_unlock', '_update_hooks', '_update_upper_cache', '_vc_info_updated', '_view_base', '_view_hash', '_view_id', '_view_use_parent_commit', '_write_queries_json', '_write_vds', 'add_data', 'add_data_from_dataframes', 'add_data_from_file', 'aggregate', 'append', 'base_storage', 'branch', 'branches', 'checkout', 'client', 'commit', 'commit_id', 'commits', 'commits_between', 'create_index', 'create_tensor', 'create_tensor_like', 'delete', 'delete_branch', 'delete_tensor', 'delete_view', 'diff', 'ds_name', 'enabled_tensors', 'extend', 'filter', 'filter_next', 'filtered_index', 'flush', 'generate_add_update_value', 'get_children_nodes', 'get_commit_details', 'get_view', 'get_views', 'group_index', 'groups', 'has_head_changes', 'index', 'indexed_tensors', 'info', 'is_first_load', 'is_head_node', 'is_iteration', 'is_optimized', 'is_view', 'libgtnf_dataset', 'link_creds', 'load_view', 'log', 'max_len', 'max_view', 'maybe_flush', 'merge', 'meta', 'min_len', 'min_view', 'no_view_dataset', 'num_samples', 'numpy', 'org_id', 'pad_tensors', 'parent', 'parse_changes', 'path', 'pending_commit_id', 'pop', 'public', 'query', 'read_only', 'rechunk', 'rename', 'reset', 'root', 'sample_indices', 'save_as_branch', 'save_view', 'set_token', 'size_approx', 'split_tensor_meta', 'statistics', 'storage', 'summary', 'tensors', 'to_arrow', 'to_json', 'to_mindrecord', 'token', 'update', 'username', 'verbose', 'version_state', 'write_to_parquet']\nkeyword_list_2 = ['__class__', '__class_getitem__', '__contains__', '__delattr__', '__delitem__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__ior__', '__iter__', '__le__', '__len__', '__lt__', '__ne__', '__new__', '__or__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__ror__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', 'clear', 'copy', 'fromkeys', 'get', 'items', 'keys', 'pop', 'popitem', 'setdefault', 'update', 'values']\nattr_and_keyword_list_3 = ['tensors', 'data', 'all_chunk_engines', 'group_index', 'cache_size', 'cache_used' 'idx',  'pg_callback', 'start_input_idx', '_client', 'path', 'storage', '_read_only_error', 'base_storage', '_read_only',  '_locked_out', 'is_iteration', 'is_first_load', '_is_filtered_view', 'index', 'group_index', 'version_state', 'pad_tensors', '_locking_enabled', '_lock_timeout', '_temp_tensors', '_commit_hooks', '_checkout_hooks', 'public', 'verbose', '_vc_info_updated', '_info', '_ds_diff', 'enabled_tensors', 'link_creds', '_update_hooks', '_view_id', '_view_base', '_view_use_parent_commit', '_pad_tensors', 'libgtnf_dataset', '_parent_dataset', '_query_string', '_inverted_index', 'filtered_index', 'split_tensor_meta', 'creds', '_vector_index', 'append_only', '_initial_autoflush', '_indexing_history', 'read_only', 'is_first_load']\n</code></pre></li> <li>For more detailed usage, please refer to <code>create_tensor()</code>. We plan to support additional <code>htype</code>s and provide clearer guidelines for tensor column creation in future releases. This documentation will be updated accordingly.</li> </ul>"},{"location":"getting_started/2_create_muller_dataset/#step-3-append-data-to-tensor-columns","title":"Step 3. Append Data to Tensor Columns","text":""},{"location":"getting_started/2_create_muller_dataset/#31-appending-a-small-number-of-samples-recommended-default-single-process-mode","title":"3.1. Appending a small number of samples (recommended: default single-process mode)","text":"<p>Append a single sample:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; with ds:  # Using the `with` context is critical for improving write performance.\n...     # Append a file by calling `muller.read()`.\n...     # Here we use the `my_photos` column (htype=image, sample_compression=jpg) as an example.\n...     # A JPEG image file can be appended directly.\n...     ds.my_photos.append(muller.read(\"path/to/file/image.jpg\"))\n...     # Alternatively, a NumPy tensor representation can be appended, as long as it\n...     # matches the expected image tensor format (H \u00d7 W \u00d7 RGB).\n...     ds.my_photos.append(np.ones((400, 300, 3), dtype=np.uint8))\n</code></pre> <p>Append multiple samples:</p> <p><pre><code>&gt;&gt;&gt; with ds:\n...     ds.my_photos.extend([\n...         muller.read(\"path/to/file/image.jpg\"),\n...         muller.read(\"path/to/file/image.jpg\")\n...     ])\n...     ds.my_text.extend([\"cat\", \"dog\", \"tree\", \"car\"])\n...     ds.my_label.extend(np.array([1, 2, 3, 4]))\n\n&gt;&gt;&gt; ds.summary()\n  tensor      htype            shape              dtype   compression\n  -------    -------          -------            -------  -----------\n  my_text     text             (4, 1)              str     None\n  my_photos   image      (4, 400, 300:500, 3)     uint8    jpeg\n  my_label    generic          (4, 1)             int32    None\n</code></pre> * Refer to [append()] and [[extend()] for detailed usage.</p>"},{"location":"getting_started/2_create_muller_dataset/#32-use-the-with-ds-context-to-improve-write-performance","title":"3.2. Use the <code>with ds:</code> context to improve write performance","text":"<p>Always wrap dataset write operations inside <code>with ds:</code>, as this can significantly improve data write throughput. For a detailed explanation, see [Using the with context to improve write performance].</p>"},{"location":"getting_started/2_create_muller_dataset/#33-appending-large-scale-data-parallel-mode","title":"3.3. Appending large-scale data (parallel mode)","text":"<p>For large-scale data ingestion, it is recommended to use the <code>@muller.compute</code> decorator to enable parallel execution (multi-threading, multi-processing, or multi-worker setups). You can also enable periodic checkpointing by setting <code>checkpoint_interval=&lt;commit_every_N_samples&gt;</code> to persist data in batches.</p> <p>See [Creating datasets and appending large-scale data] for details.</p>"},{"location":"getting_started/2_create_muller_dataset/#34-data-consistency-requirement","title":"3.4. Data consistency requirement","text":"<p>Important: When appending data column-wise, ensure that the number of newly added samples is identical across all columns. This guarantees dataset consistency and prevents mismatched column lengths.</p>"},{"location":"getting_started/2_create_muller_dataset/#2-automatic-data-ingestion-and-dataset-creation-experimental","title":"2. Automatic Data Ingestion and Dataset Creation (Experimental)","text":"<p>\u26a0\ufe0f This interface is currently experimental and may be adjusted in future releases based on user feedback.</p>"},{"location":"getting_started/2_create_muller_dataset/#option-1-converting-existing-json-csv-parquet-files-into-a-muller-dataset","title":"Option 1. Converting Existing JSON / CSV / Parquet Files into a MULLER Dataset","text":"<p>In this batch ingestion mode, the following three inputs are required:</p> <ul> <li><code>ori_path</code>: A <code>.txt</code> or <code>.json</code> file that records the source data.</li> <li><code>muller_path</code>: The target path where the MULLER dataset will be created.   The dataset name may contain letters, numbers, <code>_</code>, and <code>-</code>.</li> <li><code>schema</code>: The dataset schema, specifying column names, column types (<code>htype</code>), data types (<code>dtype</code>), and compression formats.</li> </ul>"},{"location":"getting_started/2_create_muller_dataset/#example-txt-json-file-format","title":"Example <code>.txt</code> / <code>.json</code> file format:**","text":"<pre><code>{\n    \"ori_query\": \"1. Use if-else statements to implement the following comparison function:\\nInput: in1, in2 are two 3-bit binary numbers; Output: out is a 3-bit binary number.\\nIf in1 &gt; in2, output out = 001; if in1 = in2, output out = 010; if in1 &lt; in2, output out = 100.\",\n    \"ori_response\": \"in1 = in2 = 0\\n\\nif in1 &gt; in2:\\n   out = 0b001\\nelif in1 == in2:\\n   out = 0b010\\nelse:\\n   out = 0b100\",\n    \"query_analysis\": \"{\\\"Fluency Score\\\": 5.0, \\\"Completeness Score\\\": 5.0, \\\"Complexity Score\\\": 3.0, \\\"Safety Score\\\": 5.0, \\\"Overall Quality Score\\\": 5.0, \\\"Intent Tags\\\": [{\\\"Implement Comparison Function\\\": [\\\"Use if-else statements\\\", \\\"Input: Two 3-bit binary numbers\\\", \\\"Output: One 3-bit binary number\\\", \\\"Comparison conditions: in1&gt;in2 -&gt; out=001; in1=in2 -&gt; out=010; in1&lt;in2 -&gt; out=100\\\"]}]}\",\n    \"qa_result\": \"Comprehensive Quality Analysis: The assistant's response is logically correct and accurately implements the functionality requested by the user. However, there is an issue: it does not explicitly handle the specific constraint of inputs being 3-bit binary numbers. Additionally, the response lacks input validation or error checking, which could lead to issues if incorrect data types are provided. Therefore, while grammatically and logically sound, it may be insufficient for practical hardware-logic simulation or strict data constraints.\\n\\nOverall Quality Score: 6\",\n    \"qa_score\": 6.0,\n    \"type\": \"Code Generation\"\n  },\n  {\n    \"ori_query\": \"Write a Python program that uses regular expressions to match all mobile phone numbers in a string.\\nAnalysis: First, we need to import the 're' module. Then, use the 're.findall()' function, passing in a regular expression pattern (representing a phone number) and the string to be searched. Finally, print the matched phone numbers.\",\n    \"ori_response\": \"\",\n    \"query_analysis\": \"{\\\"Fluency Score\\\": 5.0, \\\"Completeness Score\\\": 5.0, \\\"Complexity Score\\\": 2.0, \\\"Safety Score\\\": 5.0, \\\"Overall Quality Score\\\": 5.0, \\\"Intent Tags\\\": [{\\\"Write Python Program\\\": [\\\"Use regex to match mobile numbers in a string\\\", \\\"Import re module\\\", \\\"Use re.findall() function\\\", \\\"Print matched phone numbers\\\"]}]}\",\n    \"qa_result\": \"Comprehensive Quality Analysis: The assistant provided no response, so no quality analysis can be performed.\\n\\nOverall Quality Score: 0\",\n    \"qa_score\": 0.0,\n    \"type\": \"Code Generation\"\n  }\n</code></pre> <p>Example usage: <pre><code>&gt;&gt;&gt; schema_1 = {\n        'ori_query': ('text', '', None),   # If storage efficiency is not critical, we recommend leaving\n                                           # compression unset. LZ4 introduces compression/decompression\n                                           # overhead that may slightly affect I/O performance.\n        'ori_response': ('text', '', None),\n        'query_analysis': ('text', '', None),\n        'type': ('text', '', None),\n        'qa_score': ('generic', 'float32', None),\n        'qa_result': ('text', '', None),\n    }\n\n&gt;&gt;&gt; ds_1 = muller.api_dataset.create_dataset_from_file(\n        ori_path=\"example.txt\",\n        muller_path=\"my_muller_dataset/\",\n        schema=schema_1\n    )\n\n&gt;&gt;&gt; ds_1.summary()\ntensor          htype     shape    dtype     compression\n-------         -------   -------  -------   -------\nori_query        text     (2, 1)     str      None\nori_response     text     (2, 1)     str      None\nquery_analysis   text     (2, 1)     str      None\ntype             text     (2, 1)     str      None\nqa_score        generic   (2, 1)   float32    None\nqa_result        text     (2, 1)     str      None\n</code></pre></p> <p><pre><code>&gt;&gt;&gt; schema_2 = {\n        'ori_query': ('text', '', 'lz4'),\n        'ori_response': ('text', '', 'lz4'),\n        'query_analysis': {\n            'fluency_score': ('generic', 'float32', 'lz4'),\n            'completeness_score': ('generic', 'float32', 'lz4'),\n            'complexity_score': ('generic', 'float32', 'lz4'),\n            'safety_score': ('generic', 'float32', 'lz4'),\n            'overall_score': ('generic', 'float32', 'lz4'),\n            'intent_label': ('text', '', 'lz4'),\n        },\n        'type': ('text', '', 'lz4'),\n        'qa_score': ('generic', 'float32', 'lz4'),\n        'qa_result': ('text', 'str', 'lz4'),\n    }\n\n&gt;&gt;&gt; ds_2 = muller.api_dataset.create_dataset_from_file(\n        ori_path=\"example.txt\",\n        muller_path=\"my_muller_dataset/\",\n        schema=schema_2,\n        workers=0\n    )\n\n&gt;&gt;&gt; ds_2.summary()\ntensor                              htype     shape    dtype     compression\n-------                             -------   -------  -------   -------\nori_query                            text     (2, 1)     str      lz4\nori_response                         text     (2, 1)     str      lz4\nquery_analysis.fluency_score        generic   (2, 1)   float32    lz4\nquery_analysis.completeness_score   generic   (2, 1)   float32    lz4\nquery_analysis.complexity_score     generic   (2, 1)   float32    lz4\nquery_analysis.safety_score         generic   (2, 1)   float32    lz4\nquery_analysis.overall_score        generic   (2, 1)   float32    lz4\nquery_analysis.intent_label           text     (2, 1)     str      lz4\ntype                                  text     (2, 1)     str      lz4\nqa_score                            generic   (2, 1)   float32    lz4\nqa_result                             text     (2, 1)     str      lz4\n</code></pre> For detailed API specifications and file format examples, please refer to [create_dataset_from_file()]</p>"},{"location":"getting_started/2_create_muller_dataset/#option-2-converting-existing-json-csv-parquet-files-into-a-muller-dataset","title":"Option 2. Converting Existing JSON / CSV / Parquet Files into a MULLER Dataset","text":"<p>Example usage: <pre><code>&gt;&gt;&gt; dataframes = ... # The same example as in Option 1\n&gt;&gt;&gt; schema_1 = ... # The same example as in Option 1\n&gt;&gt;&gt; ds = gtn_f.api_dataset.create_dataset_from_dataframes(dataframes, \"my_gtnf_dataset/\", schema=schema_1, workers=0)\n&gt;&gt;&gt; schema_2 = ... # The same example as in Option 1\n&gt;&gt;&gt; ds = gtn_f.api_dataset.create_dataset_from_dataframes(dataframes, \"my_gtnf_dataset/\", schema=schema_2, workers=0)\n</code></pre> For detailed API specifications and file format examples, please refer to [create_dataset_from_dataframes()]</p>"},{"location":"getting_started/3_muller_dataset_operations%28load%2Cappend%2Cupdate%2Cdelete%2Cquery%29/","title":"Basic Dataset Operations (Load, Append, Update, Delete, Query)","text":""},{"location":"getting_started/3_muller_dataset_operations%28load%2Cappend%2Cupdate%2Cdelete%2Cquery%29/#1-loading-an-existing-dataset","title":"1. Loading an Existing Dataset","text":"<p>By default, the <code>main</code> branch of the dataset is loaded:</p> <pre><code>&gt;&gt;&gt; ds = muller.load(path=\"my_muller_dataset\")\n</code></pre> <p>To load a specific branch (if available), append <code>@{branch_name}</code> to the path:</p> <pre><code>&gt;&gt;&gt; ds = muller.load(path=\"my_muller_dataset@dev\")\n</code></pre> <p>To load a specific commit (if available), append <code>@{commit_id}</code> to the path:</p> <pre><code>&gt;&gt;&gt; ds = muller.load(path=\"my_muller_dataset@3e49cded62b6b335c74ff07e97f8451a37aca7b2\")\n</code></pre> <ul> <li>The usage of <code>path</code> is the same as described in [Creating an Empty Dataset]. Please add the appropriate prefix based on the storage backend.</li> <li>In addition to <code>muller.load()</code>, you may also use <code>muller.dataset()</code> to load an existing dataset (do not set overwrite=True).</li> <li>For details on branches and commit versions, see [Section 5: Version Management].</li> <li>Additional parameters and advanced usage are documented in the API reference: [muller.load()].</li> </ul>"},{"location":"getting_started/3_muller_dataset_operations%28load%2Cappend%2Cupdate%2Cdelete%2Cquery%29/#2-inspecting-dataset-metadata","title":"2. Inspecting Dataset Metadata","text":"<p>To view schema-level information for all columns in a dataset:</p> <pre><code>&gt;&gt;&gt; import muller\n&gt;&gt;&gt; ds = muller.load(\"my_muller_dataset\")\n&gt;&gt;&gt; ds.summary()\nensor     htype          shape           dtype  compression\n -------   -------        -------         -------  ------- \nmy_label   generic         (4, 1)          int32    None   \nmy_photos   image   (4, 400, 300:500, 3)   uint8    jpeg   \n my_text    text           (4, 1)           str     None\n</code></pre> <ul> <li>Usage reference: [<code>muller.summary()</code>]</li> </ul> <p>Additional APIs for inspecting dataset properties include:</p> <ul> <li>List all columns: [<code>dataset.tensors()</code>]</li> <li>Get the number of samples (rows): [<code>dataset.num_samples()</code>]</li> <li>Get the maximum and minimum lengths across columns:   [<code>dataset.max_len()</code>], [<code>dataset.min_len()</code>]</li> <li>Dataset-level statistics (e.g., per-column min / max / median / variance):   [<code>dataset.statistics()</code>]</li> </ul> <p>APIs for inspecting detailed tensor (column) metadata include:</p> <ul> <li>Column type: [<code>tensor.htype</code>]</li> <li>Column data type: [<code>tensor.dtype</code>]</li> <li>Tensor shape (column-level or per-sample):   [<code>tensor.shape_interval</code>], [<code>tensor.shape</code>]</li> <li>Tensor dimensionality: [<code>tensor.ndim</code>]</li> <li>Number of samples (rows) in a column:   [<code>tensor.num_samples</code>], [<code>tensor.__len__()</code>]</li> <li>Source file metadata for a specific sample:   [<code>tensor.sample_info</code>] (Effective for recovering image/video/audio metadata; requires <code>create_sample_info_tensor=True</code> when calling <code>create_tensor()</code>.)</li> </ul>"},{"location":"getting_started/3_muller_dataset_operations%28load%2Cappend%2Cupdate%2Cdelete%2Cquery%29/#3-adding-data","title":"3. Adding Data","text":"<p>Data can be appended to tensor columns using the same APIs and workflows described in Section 3.1 (Step 3: Adding Data to Tensor Columns).</p>"},{"location":"getting_started/3_muller_dataset_operations%28load%2Cappend%2Cupdate%2Cdelete%2Cquery%29/#4-deleting-samples-or-datasets","title":"4. Deleting Samples or Datasets","text":""},{"location":"getting_started/3_muller_dataset_operations%28load%2Cappend%2Cupdate%2Cdelete%2Cquery%29/#removing-a-sample-by-index","title":"Removing a sample by index:","text":"<pre><code>&gt;&gt;&gt; ds.pop(2)  # Removes the sample at index 2\n</code></pre>"},{"location":"getting_started/3_muller_dataset_operations%28load%2Cappend%2Cupdate%2Cdelete%2Cquery%29/#removing-multiple-samples-by-index-list","title":"Removing multiple samples by index list:","text":"<pre><code>&gt;&gt;&gt; ds.pop([1, 2, 4, 5])  # Removes samples at indices [1, 2, 4, 5]\n</code></pre>"},{"location":"getting_started/3_muller_dataset_operations%28load%2Cappend%2Cupdate%2Cdelete%2Cquery%29/#deleting-a-tensor-column-and-all-of-its-data","title":"Deleting a tensor column and all of its data:","text":"<p><pre><code>&gt;&gt;&gt; ds.delete_tensor(&lt;tensor_name&gt;)\n# Note: `large_ok` defaults to False. When deleting a column with a large\n# number of samples, set `large_ok=True` to explicitly confirm the operation\n# and prevent accidental data loss.\n</code></pre> - For detailed usage, see [<code>pop()</code>] and [<code>delete_tensor()</code>]. - Note: To ensure dataset integrity, data can only be deleted by entire rows or entire columns.</p>"},{"location":"getting_started/3_muller_dataset_operations%28load%2Cappend%2Cupdate%2Cdelete%2Cquery%29/#deleting-a-dataset-method-1","title":"Deleting a dataset (Method 1):","text":"<p>For an already loaded dataset, invoke the delete operation directly on the dataset object. <pre><code>&gt;&gt;&gt; ds.delete()\n</code></pre></p>"},{"location":"getting_started/3_muller_dataset_operations%28load%2Cappend%2Cupdate%2Cdelete%2Cquery%29/#deleting-a-dataset-method-2","title":"Deleting a dataset (Method 2):","text":"<p>For a dataset that is not currently loaded, you can delete it using the <code>muller</code> library directly. <pre><code>&gt;&gt;&gt; gtn_f.delete(path=\"/your/data/path/\", creds={'optional'})\n</code></pre> - For detailed usage, see [<code>muller.core.dataset.delete()</code>] and [<code>muller.delete()</code>].</p>"},{"location":"getting_started/3_muller_dataset_operations%28load%2Cappend%2Cupdate%2Cdelete%2Cquery%29/#5-inspecting-data-by-random-access-via-row-id-and-column-name-and-full-scan","title":"5. Inspecting Data by random access (via row id and column name) and full scan","text":""},{"location":"getting_started/3_muller_dataset_operations%28load%2Cappend%2Cupdate%2Cdelete%2Cquery%29/#access-the-i-th-row-in-a-tensor-column-first-access-the-column-then-the-row","title":"Access the i-th row in a tensor column (first access the column, then the row):","text":"<pre><code>&gt;&gt;&gt; ds.tensors\n{'my_label': Tensor(key='my_label'),\n 'my_photos': Tensor(key='my_photos'),\n 'my_text': Tensor(key='my_text')}\n\n&gt;&gt;&gt; ds.my_label[0].numpy()       # Returns the sample as a NumPy array\narray([1], dtype=int32)\n\n&gt;&gt;&gt; ds.my_label[0].data()['value']  # Returns the sample value\narray([1], dtype=int32)\n\n&gt;&gt;&gt; ds.my_label[0].tobytes()      # Returns the sample as bytes\nb'\\x01\\x00\\x00\\x00'\n</code></pre>"},{"location":"getting_started/3_muller_dataset_operations%28load%2Cappend%2Cupdate%2Cdelete%2Cquery%29/#access-multiple-rows-of-in-tensor-column-first-access-the-column-then-the-row","title":"Access multiple rows of in tensor column (first access the column, then the row):","text":"<pre><code>&gt;&gt;&gt; ds.my_label[1:4].numpy()   # Returns the sample as a NumPy array\narray([[2],\n       [3],\n       [4]], dtype=int32)\n&gt;&gt;&gt; ds.my_label[1:4].numpy(aslist=True)   # Returns the sample as a list of NumPy arrays\n[array([2], dtype=int32), array([3], dtype=int32), array([4], dtype=int32)]\n</code></pre>"},{"location":"getting_started/3_muller_dataset_operations%28load%2Cappend%2Cupdate%2Cdelete%2Cquery%29/#access-the-value-of-a-column-of-the-i-th-row-first-access-the-row-then-the-column","title":"Access the value of a column of the i-th row (first access the row, then the column):","text":"<pre><code>&gt;&gt;&gt; ds[0].my_label.numpy()\narray([1], dtype=int32)\n</code></pre>"},{"location":"getting_started/3_muller_dataset_operations%28load%2Cappend%2Cupdate%2Cdelete%2Cquery%29/#access-the-value-of-columns-in-multiple-rows-first-access-the-row-then-the-column","title":"Access the value of columns in multiple rows (first access the row, then the column):","text":"<p><pre><code>&gt;&gt;&gt; ds[1:4].my_label.numpy()  # You may alsi use .data()['value']\narray([[2],\n       [3],\n       [4]], dtype=int32)\n</code></pre> Lazy loading for rows and columns involves several <code>Tensor</code>-related APIs, each with additional optional parameters. Refer to the API documentation for more details - [[tensor.numpy()]]. Note that there are three important parameters:</p> <ul> <li> <p><code>aslist</code> (<code>bool</code>):   If <code>True</code>, returns data as a list of <code>np.ndarray</code>s. Recommended for dynamic-shape tensors.   If <code>False</code>, returns a single <code>np.ndarray</code>. May raise an error if samples have dynamic shapes.   Default: <code>False</code>.</p> </li> <li> <p><code>fetch_chunks</code> (<code>bool</code>):   If <code>True</code>, reads the entire chunk containing the sample.   If <code>False</code>, only reads the bytes needed for the sample. Exceptions: Even if <code>False</code>, it will be automatically set to <code>True</code> if:  </p> <ol> <li>The tensor is <code>ChunkCompressed</code>.  </li> <li>The chunk being accessed contains more than 128 samples.   Default can be adjusted as needed.</li> </ol> </li> <li> <p><code>asrow</code> (<code>bool</code>):   If <code>True</code>, returns samples in row-oriented format as a list of dictionaries, one dict per row.   If samples have inconsistent lengths, an error will be raised (use <code>False</code> to avoid this).   If <code>False</code>, returns data in column-oriented format as a dictionary, where each key maps to a list containing the column data.</p> </li> </ul> <ul> <li>[tensor.data()]</li> <li>[tensor.tobytes()]</li> <li>[tensor.text()]</li> <li>[tensor.dict()]</li> <li>[tensor.list()]</li> </ul>"},{"location":"getting_started/3_muller_dataset_operations%28load%2Cappend%2Cupdate%2Cdelete%2Cquery%29/#6-update-data","title":"6. Update Data","text":""},{"location":"getting_started/3_muller_dataset_operations%28load%2Cappend%2Cupdate%2Cdelete%2Cquery%29/#method-1-directly-update-the-i-th-row-in-a-given-column","title":"Method 1: Directly update the i-th row in a given column","text":"<pre><code>ds.my_tensor[i] = gtn_f.read(\"image.jpg\")\n</code></pre>"},{"location":"getting_started/3_muller_dataset_operations%28load%2Cappend%2Cupdate%2Cdelete%2Cquery%29/#method-2-update-data-by-specifying-the-column-name-and-update-values-via-the-update-api","title":"Method 2: Update data by specifying the column name and update values via the <code>update</code> API","text":"<pre><code>ds[i].update({\"my_tensor\": gtn_f.read(\"image.jpg\")})\n</code></pre> <ul> <li>Method 1 API: [tensor._setitem_()]</li> <li>Method 2 API: [dataset.update()]</li> </ul>"},{"location":"getting_started/3_muller_dataset_operations%28load%2Cappend%2Cupdate%2Cdelete%2Cquery%29/#7-query-data","title":"7. Query Data","text":"<p>MULLER provides a comprehensive suite of query functionalities tailored for AI data lakes: * Comparison Operators: Supports exact and range matching using <code>&gt;</code>,<code>&lt;</code>, <code>&gt;=</code>, and <code>&lt;=</code> for numerical types (<code>int</code>/<code>float</code>) where the tensor htype is generic. * Equality and Inequality: Supports <code>==</code> and <code>!=</code> for <code>int</code>, <code>float</code>, <code>str</code>, and <code>bool</code> types (<code>generic</code> or <code>text</code> htypes). Users can optionally build inverted indexes to significantly accelerate retrieval performance. * Full-Text Search: Supports the <code>CONTAINS</code> operator for <code>str</code> types (<code>text</code> htype), backed by an inverted index. For Chinese text, tokenization is handled by the open-source Jieba tokenizer. * Pattern Matching: Supports <code>LIKE</code> for regular expression matching on <code>str</code> types (<code>text</code> htype). * Boolean Logic: Supports complex query compositions using <code>AND</code>, <code>OR</code>, and <code>NOT</code> logical connectors. * Pagination: Supports query results with <code>OFFSET</code> and <code>LIMIT</code> clauses for efficient data sampling. * Data Aggregation: Supports standard SQL-like aggregation workflows, including <code>SELECT</code>, <code>GROUP BY</code>, and <code>ORDER BY</code>, alongside aggregate functions such as <code>COUNT</code>, <code>AVG</code>, <code>MIN</code>, <code>MAX</code>, and <code>SUM</code>. * Vector Similarity Search: Supports high-dimensional vector similarity retrieval based on IVFPQ, HNSW and DISKANN for AI-centric embedding analysis.</p>"},{"location":"getting_started/3_muller_dataset_operations%28load%2Cappend%2Cupdate%2Cdelete%2Cquery%29/#example-1-exact-match-query-without-index-acceleration","title":"Example 1: Exact Match Query (without index acceleration)","text":"<p>Retrieve all values in the <code>test1</code> column (of type <code>generic</code>) that are greater than 2. <pre><code>&gt;&gt;&gt; ds = gtn_f.dataset(\"temp_test\", overwrite=True)\n&gt;&gt;&gt; ds.create_tensor(name=\"test1\", htype=\"generic\")   \n&gt;&gt;&gt; ds.test1.extend(np.random.randint(5, size=10000))\n&gt;&gt;&gt; ds_1 = ds.filter_vectorized([(\"test1\", \"&gt;\", 2)]) #Note: If the fourth parameter is not specified, it defaults to `False`, which is equivalent to the usage in the next line.\n&gt;&gt;&gt; ds_1 = ds.filter_vectorized([(\"test1\", \"&gt;\", 2, False)])\n&gt;&gt;&gt; ds_1.test1.numpy()\narray([[4],\n       [3],\n       [4],\n       ...,\n       [4],\n       [3],\n       [4]], shape=(3922, 1))\n&gt;&gt;&gt; ds_1.filtered_index\n[np.int64(1),\n np.int64(3),\n np.int64(5),\n np.int64(7), ...]\n</code></pre></p>"},{"location":"getting_started/3_muller_dataset_operations%28load%2Cappend%2Cupdate%2Cdelete%2Cquery%29/#example-2-exact-match-query-without-index-acceleration-negation-not","title":"Example 2: Exact Match Query (without index acceleration) \u2014 Negation (NOT)","text":"<p>Retrieve all values in the <code>test1</code> column (of type <code>generic</code>) that do NOT satisfy the condition <code>&gt; 2</code>. <pre><code>&gt;&gt;&gt; ds = gtn_f.dataset(\"temp_test\", overwrite=True)\n&gt;&gt;&gt; ds.create_tensor(name=\"test1\", htype=\"generic\")   \n&gt;&gt;&gt; ds.test1.extend(np.random.randint(5, size=10000))\n&gt;&gt;&gt; ds_1 = ds.filter_vectorized([(\"test1\", \"&gt;\", 2, False, \"NOT\")])\n&gt;&gt;&gt; ds_1.test1.numpy()\narray([[2],\n       [0],\n       [0],\n       ...,\n       [1],\n       [2],\n       [0]], shape=(6078, 1))\n</code></pre></p>"},{"location":"getting_started/3_muller_dataset_operations%28load%2Cappend%2Cupdate%2Cdelete%2Cquery%29/#example-3-exact-match-query-with-index-acceleration","title":"Example 3: Exact Match Query (with Index Acceleration)","text":"<p>Retrieve all values in the <code>test1</code> column (of type <code>generic</code>) that are equal to 2.</p> <p>Note: Index creation only applies to data that has been recorded in a commit. Before creating an index, ensure that <code>ds.commit()</code> is executed to consolidate all pending changes into a commit version. <pre><code>&gt;&gt;&gt; ds = gtn_f.dataset(\"temp_test\", overwrite=True)\n&gt;&gt;&gt; ds.create_tensor(name=\"test1\", htype=\"generic\")   \n&gt;&gt;&gt; ds.test1.extend(np.random.randint(5, size=10000))\n&gt;&gt;&gt; ds.commit()\n&gt;&gt;&gt; ds.create_index([\"test1\"]) # Create inverted index\n&gt;&gt;&gt; ds_1 = ds.filter_vectorized([(\"test1\", \"==\", 2, True)])\n&gt;&gt;&gt; ds_1.test1.numpy()\narray([[2],\n       [2],\n       [2],\n       ...,\n       [2],\n       [2],\n       [2]], shape=(2006, 1))\n</code></pre></p>"},{"location":"getting_started/3_muller_dataset_operations%28load%2Cappend%2Cupdate%2Cdelete%2Cquery%29/#example-4-exact-match-query-without-index-acceleration","title":"Example 4: Exact Match Query (without Index Acceleration)","text":"<p>Retrieve all values in the <code>test2</code> column (of type <code>text</code>) that are equal to <code>\"hi\"</code>. <pre><code>&gt;&gt;&gt; ds = gtn_f.dataset(\"temp_test\", overwrite=True)\n&gt;&gt;&gt; ds.create_tensor(name=\"test2\",htype=\"text\")\n&gt;&gt;&gt; with ds:\n         ds.test2.extend([\"hi\", \"bye\", \"oops\", \"hello\", \"world\"]*2000)\n&gt;&gt;&gt; ds_2 = ds.filter_vectorized([(\"test2\", \"==\", \"hi\" )])\n&gt;&gt;&gt; ds_2.test2.numpy()\narray([['hi'],\n       ['hi'],\n       ['hi'],\n       ...,\n       ['hi'],\n       ['hi'],\n       ['hi']], shape=(2000, 1), dtype='&lt;U2')\n</code></pre></p>"},{"location":"getting_started/3_muller_dataset_operations%28load%2Cappend%2Cupdate%2Cdelete%2Cquery%29/#example-5-exact-match-query-with-index-acceleration","title":"Example 5: Exact Match Query (with Index Acceleration)","text":"<p>Retrieve all values in the <code>test2</code> column (of type <code>text</code>) that are equal to <code>\"hi\"</code>. <pre><code>&gt;&gt;&gt; ds = gtn_f.dataset(\"temp_test\", overwrite=True)\n&gt;&gt;&gt; ds.create_tensor(name=\"test2\",htype=\"text\")\n&gt;&gt;&gt; with ds:\n         ds.test2.extend([\"hi\", \"bye\", \"oops\", \"hello\", \"world\"]*2000)\n&gt;&gt;&gt; ds.commit()\n&gt;&gt;&gt; ds.create_index([\"test2\"])\n&gt;&gt;&gt; ds_2 = ds.filter_vectorized([(\"test2\", \"==\", \"hi\", True)])\n&gt;&gt;&gt; ds_2.test2.numpy()\narray([['hi'],\n       ['hi'],\n       ['hi'],\n       ...,\n       ['hi'],\n       ['hi'],\n       ['hi']], shape=(2000, 1), dtype='&lt;U2')\n</code></pre></p>"},{"location":"getting_started/3_muller_dataset_operations%28load%2Cappend%2Cupdate%2Cdelete%2Cquery%29/#example-6-exact-match-query-with-logical-connectors-and-or-not","title":"Example 6: Exact Match Query with Logical Connectors (AND, OR, NOT)","text":"<p>Perform an exact-match query combining multiple conditions using logical connectors.  </p> <p>Note: The number of elements in <code>connector_list</code> must be one less than the number of elements in <code>condition_list</code>. <pre><code>&gt;&gt;&gt; ds = gtn_f.dataset(\"temp_test\", overwrite=True)\n&gt;&gt;&gt; ds.create_tensor(name=\"test3\", htype=\"generic\")\n&gt;&gt;&gt; ds.test3.extend(np.random.randint(5, size=10000))\n&gt;&gt;&gt; ds.create_tensor(name=\"test4\", htype=\"generic\")\n&gt;&gt;&gt; ds.test4.extend(np.random.randint(100, size=10000))\n&gt;&gt;&gt; ds_3 = ds.filter_vectorized(condition_list=[(\"test3\", \"&gt;\", 2), (\"test3\", \"&lt;=\", 4), (\"test4\", \"&lt;\", 60, False, \"NOT\")], connector_list=[\"AND\", \"OR\"]) \n&gt;&gt;&gt; len(ds_3)\n6319\n</code></pre></p>"},{"location":"getting_started/3_muller_dataset_operations%28load%2Cappend%2Cupdate%2Cdelete%2Cquery%29/#example-7-exact-match-query-with-offset-and-limit","title":"Example 7: Exact Match Query with Offset and Limit","text":"<p>Perform an exact-match query while applying <code>offset</code> and <code>limit</code> to control the subset of returned results. <pre><code>&gt;&gt;&gt; ds = gtn_f.dataset(\"temp_test\", overwrite=True)\n&gt;&gt;&gt; ds.create_tensor(name=\"test5\", htype=\"generic\")\n&gt;&gt;&gt; ds.test5.extend(np.arange(0, 100))\n&gt;&gt;&gt; ds_5 = ds.filter_vectorized([(\"test5\", \"&lt;\", 50), (\"test5\", \"&gt;=\", 20)], [\"AND\"], offset=30, limit=10)  # Start the query from row 60 and return only 10 results.\n&gt;&gt;&gt; len(ds_5)\n10\n</code></pre></p>"},{"location":"getting_started/3_muller_dataset_operations%28load%2Cappend%2Cupdate%2Cdelete%2Cquery%29/#example-8-text-keyword-search-index-required","title":"Example 8: Text Keyword Search (Index Required)","text":"<p>Perform a keyword-based search on text columns.  </p> <p>(1) English keyword query</p> <pre><code>&gt;&gt;&gt; ds = gtn_f.dataset(\"temp_test\", overwrite=True)\n&gt;&gt;&gt; ds.create_tensor(name=\"test6\", htype=\"text\")\n&gt;&gt;&gt; with ds:\n         ds.test6.extend([\"A majestic long-haired Maine Coon cat perched on a wooden bookshelf, staring intently at a tree outside with its bright amber eyes.\",\n                          \"A soft, white lop-eared rabbit with bright eyes nestled in a patch of clover, twitching its pink nose while nibbling on a fresh garden carrot.\",\n                          \"A focused German Shepherd sitting patiently on a cobblestone street, wearing a professional service harness and looking up at its handler for the next command.\",\n                          \"A domestic short-hair cat with a distinctive tuxedo pattern stretching lazily across a velvet sofa in a dimly lit living room.\"]*2500)\n&gt;&gt;&gt; ds.commit()\n&gt;&gt;&gt; ds.create_index([\"test6\"]) \n&gt;&gt;&gt; ds_6 = ds.filter_vectorized([(\"test6\", \"CONTAINS\", \"bright eyes\")])\n&gt;&gt;&gt; ds_6.test6[:2].data()[\"value\"]\n[np.str_('A majestic long-haired Maine Coon ... with its bright amber eyes.'),\n np.str_('A soft, white lop-eared rabbit with bright eyes...'), ...]\n# Jointly query\n&gt;&gt;&gt; ds_7 = ds.filter_vectorized([(\"test5\", \"&lt;\", 50), (\"test5\", \"&gt;=\", 20), (\"test6\", \"CONTAINS\", \"\u4e2d\u5c71\u5927\u5b66\")], [\"AND\", \"OR\"], limit=10)\n</code></pre> <p>(2) Chinese keywrod query - The tokenization is implemented using the Chinese word segmentation tool jieba.</p> <pre><code>&gt;&gt;&gt; ds = gtn_f.dataset(\"temp_test\", overwrite=True)\n&gt;&gt;&gt; ds.create_tensor(name=\"test6\", htype=\"text\")\n&gt;&gt;&gt; with ds:\n         ds.test6.extend([\"\u6211\u6bd5\u4e1a\u4e8e\u9999\u6e2f\u79d1\u6280\u5927\u5b66\u3002\", \"\u6211\u6bd5\u4e1a\u4e8e\u4e2d\u5c71\u5927\u5b66\u3002\"]*5000)\n&gt;&gt;&gt; ds.commit()\n&gt;&gt;&gt; ds.create_index([\"test6\"]) #\u5012\u6392\u7d22\u5f15\n&gt;&gt;&gt; ds_6 = ds.filter_vectorized([(\"test6\", \"CONTAINS\", \"\u4e2d\u5c71\u5927\u5b66\")])\n&gt;&gt;&gt; ds_6.test6[:2].data()[\"value\"]\n[np.str_('\u6211\u6bd5\u4e1a\u4e8e\u4e2d\u5c71\u5927\u5b66\u3002'),\n np.str_('\u6211\u6bd5\u4e1a\u4e8e\u4e2d\u5c71\u5927\u5b66\u3002'), ...]\n# Jointly query\n&gt;&gt;&gt; ds_7 = ds.filter_vectorized([(\"test5\", \"&lt;\", 50), (\"test5\", \"&gt;=\", 20), (\"test6\", \"CONTAINS\", \"\u4e2d\u5c71\u5927\u5b66\")], [\"AND\", \"OR\"], limit=10)\n</code></pre>"},{"location":"getting_started/3_muller_dataset_operations%28load%2Cappend%2Cupdate%2Cdelete%2Cquery%29/#example-9-regex-based-text-matching-using-like","title":"Example 9: Regex-Based Text Matching (Using <code>LIKE</code>)","text":"<p>Perform text matching using regular expressions, with <code>LIKE</code> specified as the query operator. <pre><code>&gt;&gt;&gt; ds = gtn_f.dataset(\"temp_test\", overwrite=True)\n&gt;&gt;&gt; ds.create_tensor(name=\"test7\", htype=\"text\")\n&gt;&gt;&gt; ds.test7.extend(['A0', 'A1', 'A2', 'A3', 'A4', 'B0', 'B1', 'C0'])\n&gt;&gt;&gt; ds_8 = ds.filter_vectorized([(\"test7\", \"LIKE\", \"A[0-2]\")])\n&gt;&gt;&gt; ds_8.test7.numpy()\narray([['A0'],\n       ['A1'],\n       ['A2']], dtype='&lt;U2')\n</code></pre></p>"},{"location":"getting_started/3_muller_dataset_operations%28load%2Cappend%2Cupdate%2Cdelete%2Cquery%29/#example-11-aggregation-and-group-by-statistics","title":"Example 11: Aggregation and Group-By Statistics","text":"<p>Perform aggregation operations equivalent to the SQL statement:</p> <p><pre><code>select ori_query, ori_response, count(*)\nfrom dataset\ngroup by ori_query, ori_response\norder by ori_query;\n</code></pre> <pre><code>&gt;&gt;&gt; ds = gtn_f.dataset(\"temp_test\", overwrite=True)\n&gt;&gt;&gt; tensors = [\"ori_query\", \"ori_response\", \"query_analysis\", \"result\", \"score\", \"type\"]\n&gt;&gt;&gt; ds.create_tensor(\"ori_query\", htype=\"text\", exist_ok=True)\n&gt;&gt;&gt; ds.create_tensor(\"ori_response\", htype=\"text\", exist_ok=True)\n&gt;&gt;&gt; ds.create_tensor(\"query_analysis\", htype=\"text\", exist_ok=True)\n&gt;&gt;&gt; ds.create_tensor(\"result\", htype=\"text\", exist_ok=True)\n&gt;&gt;&gt; ds.create_tensor(\"score\", htype=\"generic\", exist_ok=True, dtype=\"float64\")\n&gt;&gt;&gt; ds.create_tensor(\"type\", htype=\"generic\", exist_ok=True, dtype=\"float64\")\n&gt;&gt;&gt; np_data = np.array([\n  [\n    \"1. Use if-else statements to implement comparison logic\",\n    \"if in1 &gt; in2:\",\n    \"{\\\"Fluency Score\\\": 5.0, \\\"Completeness Score\\\": 3.0}\",\n    \"Overall Quality Score: 6\",\n    6.0,\n    5.0\n  ],\n  [\n    \"Write a Python program that uses regular expressions to match mobile phone numbers.\",\n    \"First, we need to import the re module.\",\n    \"{\\\"Fluency Score\\\": 5.0, \\\"Completeness Score\\\": 2.0}\",\n    \"Overall Quality Score: 0\",\n    0.0,\n    4.0\n  ],\n  [\n    \"Compile in C language: calculate the surface area and volume of a cylinder\",\n    \"Four variables are defined: `r`, `h`, `surface_area`, `volume`\",\n    \"{\\\"Fluency Score\\\": 5.0, \\\"Completeness Score\\\": 4.0}\",\n    \"Overall Quality Score: 6\",\n    6.0,\n    6.0\n  ],\n  [\n    \"How to combine the three RGB channel values into a single RGB value\",\n    \"In Java, the extracted R, G, and B channel values are added together to form an RGB value.\",\n    \"{\\\"Fluency Score\\\": 5.0, \\\"Completeness Score\\\": 4.0}\",\n    \"Overall Quality Score: 7\",\n    7.0,\n    4.0\n  ],\n  [\n    \"How to combine the three RGB channel values into a single RGB value\",\n    \"In Java, the extracted R, G, and B channel values are added together to form an RGB value.\",\n    \"{\\\"Fluency Score\\\": 5.0, \\\"Completeness Score\\\": 4.0}\",\n    \"Overall Quality Score: 7\",\n    7.0,\n    4.0\n  ],\n  [\n    \"Design an intelligent customer service system based on deep learning\",\n    \"A deep learning\u2013based intelligent customer service system includes steps such as data preprocessing, model construction, model training, and prediction.\",\n    \"{\\\"Fluency Score\\\": 5.0, \\\"Completeness Score\\\": 5.0}\",\n    \"Overall Quality Score: 6\",\n    6.0,\n    3.0\n  ]\n])\n&gt;&gt;&gt; for i, item in enumerate(tensors):\n         ds[item].extend(np_data[:, i].astype(ds[item].dtype))\n&gt;&gt;&gt; result2 = ds.aggregate_vectorized(\n        group_by_tensors=['ori_query', 'ori_response'],\n        selected_tensors=['ori_query', 'ori_response'],\n        order_by_tensors=['ori_query'],\n        aggregate_tensors=[\"*\"],\n        )\n&gt;&gt;&gt; result2\narray([[\"Write a Python program that uses regular expressions to match mobile phone numbers.\", \"First, we need to import the re module.\", '1'],\n       [\"Compile in C language: calculate the surface area and volume of a cylinder\", \"Four variables are defined: `r`, `h`, `surface_area`, `volume`\", '1'],\n       [\"How to combine the three RGB channel values into a single RGB value\", \"In Java, the extracted R, G, and B channel values are added together to form an RGB value.\", '2'],\n       [\"Design an intelligent customer service system based on deep learning\", \"A deep learning\u2013based intelligent customer service system includes steps such as data preprocessing, model construction, model training, and prediction.\", '1'],\n       [\"1. Use if-else statements to implement comparison logic\", \"if in1 &gt; in2:\", '1']], dtype='&lt;U38')\n</code></pre> * For detailed usage, please refer to the following API documentation:</p> <ul> <li>filter_vectorized(): vectorized query interface for conditional filtering</li> <li>aggregate_vectorized(): vectorized aggregation interface (e.g., group by + count(*))</li> <li>create_index(): interface for creating inverted indexes</li> </ul>"},{"location":"getting_started/3_muller_dataset_operations%28load%2Cappend%2Cupdate%2Cdelete%2Cquery%29/#example-12-vector-search","title":"Example 12: Vector Search","text":"<pre><code># Create a sample dataset\nimport numpy as np\nds_vec = muller.dataset(path=\"test_data_vec/\", overwrite=True)\nds_vec.create_tensor(name=\"embeddings\", htype=\"vector\", dtype=\"float32\", dimension=32)\nds_vec.embeddings.extend(np.random.rand(320000).reshape(10000, 32).astype(np.float32))\n\n# Create index\nds_vec.commit()\nds_vec.create_vector_index(\"embeddings\", index_name=\"flat\", index_type=\"FLAT\", metric=\"l2\")\nds_vec.create_vector_index(\"embeddings\", index_name=\"hnsw\", index_type=\"HNSWFLAT\", metric=\"l2\", ef_construction=40, m=32)\n\n# Vector search\nq = np.random.rand(1000, 32)\nds_vec.load_vector_index(\"embeddings\", index_name=\"flat\")\nres_7 = ds_vec.vector_search(query_vector=q, tensor_name=\"embeddings\", index_name=\"flat\", topk=1)\n_, ground_truth = res_7\n\nds_vec.load_vector_index(\"embeddings\", index_name=\"hnsw\")\nres_8 = ds_vec.vector_search(query_vector=q, tensor_name=\"embeddings\", index_name=\"hnsw\", ef_search=16)\n_, res_id = res_8\n\n# Compute the recall\nrecall = np.ones(len(res_id))[(ground_truth==res_id).flatten()].sum() / len(res_id)\n</code></pre>"},{"location":"getting_started/3_muller_dataset_operations%28load%2Cappend%2Cupdate%2Cdelete%2Cquery%29/#8-saving-and-loading-dataset-views","title":"8. Saving and Loading Dataset Views","text":"<p>Each query result can be persisted as a materialized view and assigned a unique ID. For subsequent identical queries, the corresponding materialized view can be loaded directly using this ID, avoiding redundant computation. <pre><code>my_view = ds.filter([(\"lable\", \"==\", 1)])\nmy_view.save_view(id='my_query_id')  # Note: need to specify the `id`!\nmy_view = ds.load_view(id='my_query_id') # Note: need to specify the `id`!\n</code></pre></p> <p>Materialized view\u2013related APIs support additional parameters to optimize read and write performance, such as <code>optimize</code> and <code>num_workers</code>. For details, please refer to the view-related API documentation.</p>"},{"location":"getting_started/3_muller_dataset_operations%28load%2Cappend%2Cupdate%2Cdelete%2Cquery%29/#9-other-dataset-related-apis","title":"9. Other Dataset-Related APIs","text":"<p>For the complete API reference, see [Datasets] and its subpages. Key APIs include:</p> <ul> <li>Dataset copying: <ul> <li>Full copy including all branches: [dataset.deepcopy()]</li> <li>Copy only the latest commit on the main branch: [dataset.copy()]</li> </ul> </li> <li>Dataset rechunking (optimize chunk sizes for each tensor): [dataset.rechunk()]</li> <li>Export dataset to MindRecord: [dataset.to_mindrecord()]</li> <li>Export dataset to JSON: [dataset.to_json()]</li> <li>Export dataset to Arrow: [dataset.to_arrow()]</li> <li>Convert dataset to a pandas DataFrame (for inspection and visualization): [dataset.to_dataframe()]</li> </ul>"},{"location":"getting_started/4_muller_version_control/","title":"4 muller version control","text":""},{"location":"getting_started/4_muller_version_control/#version-control","title":"Version Control","text":"<p>MULLER provides Git-like commands to manage dataset changes. It works with datasets of any size and records key information about dataset evolution. For the full API reference, see Dataset Version Control.</p> <p>Below are several key commands.</p>"},{"location":"getting_started/4_muller_version_control/#1-commit","title":"1. Commit","text":"<p>After performing write operations (add/delete/update), call <code>ds.commit()</code> to create a new version (commit ID).</p> <p>Example: load a dataset, create a tensor column, and commit.</p> <pre><code>&gt;&gt;&gt; ds = muller.dataset(\"/data/muller_exp/\", overwrite=True)\n&gt;&gt;&gt; ds.create_tensor(name=\"labels\", htype=\"generic\", dtype=\"int\")\n&gt;&gt;&gt; first_commit_id = ds.commit(message=\"first commit.\")\n&gt;&gt;&gt; first_commit_id\n'firstdbf9474d461a19e9333c2fd19b46115348f'\n</code></pre> <p>You can then continue to modify the dataset and commit again.</p> <pre><code>&gt;&gt;&gt; ds.labels.extend([1, 2, 3, 4, 5])\n&gt;&gt;&gt; second_commit_id = ds.commit(message=\"add labels.\")\n&gt;&gt;&gt; second_commit_id\n'637adeb5232f0152b866c9a2a49e8da19f00c1da'\n</code></pre> <ul> <li>For details, see <code>dataset.commit()</code>.</li> </ul>"},{"location":"getting_started/4_muller_version_control/#2-checkout","title":"2. Checkout","text":"<p>Each newly created dataset starts on the <code>main</code> branch. Use <code>ds.checkout()</code> to create a branch or switch branches.</p> <p>Create a new branch with <code>create=True</code>:</p> <pre><code>&gt;&gt;&gt; ds.checkout(\"dev\", create=True)\n&gt;&gt;&gt; ds.create_tensor(\"categories\", htype=\"text\")\n&gt;&gt;&gt; ds.categories.extend([\"agent\", \"emotion\", \"generation\", \"writing\", \"emotion\"])\n&gt;&gt;&gt; ds.commit(\"created categories tensor column and add values.\")\n&gt;&gt;&gt; ds.summary()  # Now dev has two tensor columns (categories and labels).\ntensor     htype    shape    dtype  compression\n -------    -------  -------  -------  -------\ncategories   text    (5, 1)     str     None\n  labels    generic  (5, 1)    int64    None\n</code></pre> <p>Switch to an existing branch:</p> <pre><code>&gt;&gt;&gt; ds.checkout(\"main\")  # Switch back to main.\n&gt;&gt;&gt; ds.branch            # Check the current branch name.\n'main'\n&gt;&gt;&gt; ds.summary()         # This branch has only one tensor column (labels).\ntensor    htype    shape    dtype  compression\n-------  -------  -------  -------  -------\nlabels   generic  (5, 1)    int64    None\n</code></pre> <ul> <li>For details, see <code>dataset.checkout()</code>.</li> <li>For details, see <code>dataset.branch</code>.</li> </ul>"},{"location":"getting_started/4_muller_version_control/#3-load-a-specific-branchversion-and-view-history","title":"3. Load a Specific Branch/Version and View History","text":"<p>Use <code>muller.load()</code> to load a specific branch or commit.</p> <p>Load the latest commit on <code>main</code> (default):</p> <pre><code>ds = muller.load(path=\"/data/muller_exp/\")\n</code></pre> <p>Load a specific branch using <code>@{branch_name}</code>:</p> <pre><code>ds = muller.load(path=\"/data/muller_exp@dev\")\n</code></pre> <p>Load a specific commit using <code>@{commit_id}</code>:</p> <pre><code>ds = muller.load(path=\"/data/muller_exp@3e49cded62b6b335c74ff07e97f8451a37aca7b2\")\n</code></pre> <p><code>ds.log()</code> prints the log to the console and returns commit records starting from the current version.</p> <pre><code>&gt;&gt;&gt; ds = muller.load(path=\"/data/muller_exp@dev\")\n&gt;&gt;&gt; ds.log()\n---------------\nMULLER Version Log\n---------------\n\nCurrent Branch: dev\nCommit : 476be1766915dfe652f39e39f5370f9c04528df9 (dev)\nAuthor : public\nTime   : 2025-02-28 08:07:48\nMessage: created categories tensor column and add values.\n\nCommit : 637adeb5232f0152b866c9a2a49e8da19f00c1da (main)\nAuthor : public\nTime   : 2025-02-28 08:03:16\nMessage: add labels.\n\nCommit : firstdbf9474d461a19e9333c2fd19b46115348f (main)\nAuthor : public\nTime   : 2025-02-28 03:32:53\nMessage: first commit.\n</code></pre> <ul> <li>For details, see <code>dataset.log()</code>.</li> <li>If needed, we may extend <code>log()</code> to also return the history of commits merged into the current branch.</li> <li>Other related APIs:</li> <li>Get commit details: <code>dataset.get_commit_details(commit_id)</code></li> <li>Get current commit ID: <code>dataset.commit_id</code></li> <li>Get the next (pending) commit ID: <code>dataset.pending_commit_id</code></li> <li>Check whether the current branch has uncommitted changes: <code>dataset.has_head_changes</code></li> <li>List all commits (without printing): <code>dataset.commits</code></li> <li>List all branches: <code>dataset.branches</code></li> <li>Get commits between a version and a branch: <code>dataset_commits_between()</code></li> </ul>"},{"location":"getting_started/4_muller_version_control/#4-direct-diff-available-in-v0610","title":"4. Direct Diff (available in v0.6.10+)","text":"<p>Understanding changes between versions is critical. MULLER provides <code>ds.direct_diff(id_1, id_2)</code> to compute the direct per-tensor, per-row differences between <code>id_1</code> and <code>id_2</code> (in the direction \u201cfrom <code>id_1</code> to <code>id_2</code>\u201d). It can optionally return a pandas DataFrame for inspection.</p> <p>Parameters (note that the order matters):</p> <ul> <li>id_1 (str): the first version ID or branch name.</li> <li>id_2 (str): the second version ID or branch name.</li> <li>as_dataframe (bool, optional): if <code>True</code>, return DataFrame(s).</li> <li>force (bool, optional): when the resulting DataFrame is large (over 100,000 rows by default), set <code>force=True</code> to confirm execution.</li> </ul> <p>Example:</p> <pre><code>import muller\n\nds = muller.dataset(path=\"temp_test\", overwrite=True)\n\nds.create_tensor(name=\"labels\", htype=\"generic\", dtype=\"int\")\nds.create_tensor(name=\"categories\", htype=\"text\")\nds.create_tensor(name=\"test1\", htype=\"generic\", dtype=\"int\")\nds.labels.extend([0, 1, 2, 3, 4])\nds.categories.extend([\"a\", \"b\", \"c\", \"d\", \"e\"])\nds.test1.extend([100, 101, 102, 103, 104])\n\nds.checkout(\"dev\", create=True)\nds.labels[0] = 11\nds.categories[3] = \"haha\"\nds.pop(2)\nds.labels.append(100)\nds.categories.append(\"hello\")\nds.test1.append(105)\ndev_1 = ds.commit(\"first on dev\")\n\nds.checkout(\"main\", create=False)\nds.checkout(\"dev_2\", create=True)\nds.delete_tensor(\"test1\")\nds.labels.extend([5, 6])\nds.categories.extend([\"f\", \"g\"])\nds.commit(\"first on dev_2\")\nds.labels[1] = 111\nds.categories[1] = \"xixixi\"\nds.create_tensor(name=\"test2\", htype=\"generic\", dtype=\"int\")\nds.test2.extend([100, 101, 102, 103, 104, 105, 106])\ndev_2 = ds.commit(\"sec on dev_2\")\n\nfinal_df_dict = ds.direct_diff(dev_1, dev_2, as_dataframe=True)\n</code></pre> <p>In a Jupyter environment, you can view the changes visually. </p>"},{"location":"getting_started/4_muller_version_control/#5-diff","title":"5. Diff","text":"<p><code>ds.diff()</code> computes differences across versions for each tensor column and each sample (row). This API primarily supports merge computation, so its output is not a simple \u201cabsolute diff\u201d; it returns diffs per commit relative to the most recent common ancestor (a version-tree style output, similar to <code>log()</code>).</p> <p>Notes:</p> <ul> <li>If you call <code>diff()</code> while you are on one of the versions being compared and your current state is the branch HEAD, then uncommitted HEAD changes will also be included.</li> <li>By default, <code>diff()</code> returns row indices only. To return actual values, set <code>show_value=True</code>. Since loading values requires I/O, consider using <code>offset</code> and <code>limit</code> for large datasets, especially when the data is stored remotely.</li> <li>Use <code>as_dict=True</code> to return diffs as a Python dict.</li> </ul> <p>Common usages:</p> <p>1) Diff between the current state and the previous commit:</p> <pre><code>ds.diff()  # If nothing changed since the last commit, nothing will be returned.\n</code></pre> <p>2) Diff between the current state and <code>id_1</code>:</p> <pre><code>ds.diff(id_1=&lt;commit_id&gt;)      # id_1 can be a commit ID\nds.diff(id_1=&lt;branch_name&gt;)    # id_1 can be a branch name (its latest commit)\n</code></pre> <p>3) Diff between <code>id_1</code> and <code>id_2</code>:</p> <pre><code>ds.diff(id_1=&lt;commit_id_1&gt;, id_2=&lt;commit_id_2&gt;)          # commit IDs\nds.diff(id_1=&lt;branch_name_1&gt;, id_2=&lt;branch_name_2&gt;)      # branch names (latest commits)\n</code></pre> <p>Key parameters:</p> <ul> <li>id_1 (str, optional): first version ID or branch name.</li> <li>id_2 (str, optional): second version ID or branch name.</li> <li>as_dict (bool, optional): if <code>True</code>, return structured diffs as Python objects.</li> <li>show_value (bool): if <code>True</code>, return actual values for append/update/pop.</li> <li>offset (int): number of items to skip (effective only when <code>show_value=True</code>).</li> <li>limit (int): maximum number of items to show (effective only when <code>show_value=True</code>, default 1000).</li> <li>asrow (bool): return values row-wise (list of dicts) when <code>show_value=True</code>. This is only applicable when tensor-count and row-count changes are exactly aligned between the two versions; otherwise it may raise an error. In general, prefer <code>asrow=False</code> for flexibility.</li> </ul> <p>Example (based on the dataset created in Section 5.2): show indices only.</p> <pre><code>&gt;&gt;&gt; ds.diff(\"dev\", \"main\")  # default: do not show values\n## MULLER Diff\nThe 2 diffs are calculated relative to the most recent common ancestor (637adeb5232f0152b866c9a2a49e8da19f00c1da) of the two commits passed.\n------------------------------------------------------------------------------------------------------------------------\nDiff in dev (target id 1):\n\n********************************************************************************\ncommit UNCOMMITTED HEAD\nAuthor: public\nDate: None\nMessage: None\n\nNo changes were made in this commit.\n********************************************************************************\ncommit 476be1766915dfe652f39e39f5370f9c04528df9\nAuthor: public\nDate: 2025-02-28 08:07:48\nMessage: created categories tensor column and add values.\n\ncategories\n* Created tensor\n* Added 5 samples: [0-5]\n\n------------------------------------------------------------------------------------------------------------------------\nDiff in main (target id 2):\n\n********************************************************************************\ncommit UNCOMMITTED HEAD\nAuthor: public\nDate: None\nMessage: None\n\nNo changes were made in this commit.\n------------------------------------------------------------------------------------------------------------------------\n{}\n&gt;&gt;&gt;\n</code></pre> <p>Example: show actual values.</p> <pre><code>&gt;&gt;&gt; ds.diff(id_1=third_commit_id, id_2=first_commit_id, show_value=True)\n## MULLER Diff\nThe 2 diffs are calculated relative to the most recent common ancestor (firstdbf9474d461a19e9333c2fd19b46115348f) of the two commits passed.\n------------------------------------------------------------------------------------------------------------------------\nDiff in 51f5bb41c4a1d74d881269366e3718285b8145db (target id 1):\n\n********************************************************************************\ncommit 51f5bb41c4a1d74d881269366e3718285b8145db\nAuthor: public\nDate: 2025-03-03 04:03:13\nMessage: created categories tensor column and add values.\n\ncategories\n* Created tensor\n* Added 5 samples: [0-5],\nThe appended data values are {'created': True, 'cleared': False, 'info_updated': False, 'data_added': [0, 5], 'data_updated': set(), 'data_deleted': SortedSet([]), 'data_deleted_ids': [], 'data_transformed_in_place': False, 'add_value': [array(['agent'], dtype='&lt;U5'), array(['emotion'], dtype='&lt;U7'), array(['generation'], dtype='&lt;U10'), array(['writing'], dtype='&lt;U7'), array(['emotion'], dtype='&lt;U7')], 'updated_values': [], 'data_deleted_values': []}\n\n********************************************************************************\ncommit 6536edb5ee077de334d6f23133bcf7e209d1be80\nAuthor: public\nDate: 2025-03-03 04:03:12\nMessage: add labels.\n\nlabels\n* Added 5 samples: [0-5],\nThe appended data values are {'created': False, 'cleared': False, 'info_updated': False, 'data_added': [0, 5], 'data_updated': set(), 'data_deleted': SortedSet([]), 'data_deleted_ids': [], 'data_transformed_in_place': False, 'add_value': [array([1]), array([2]), array([3]), array([4]), array([5])], 'updated_values': [], 'data_deleted_values': []}\n\n------------------------------------------------------------------------------------------------------------------------\nDiff in firstdbf9474d461a19e9333c2fd19b46115348f (target id 2):\n\nNo changes were made.\n\n------------------------------------------------------------------------------------------------------------------------\n{}\n</code></pre> <p>Example: return values as a dict.</p> <pre><code>&gt;&gt;&gt; ds.diff(id_1=third_commit_id, id_2=first_commit_id, as_dict=True, show_value=True)\n{'dataset': ([{'commit_id': '51f5bb41c4a1d74d881269366e3718285b8145db',\n    'author': 'public',\n    'message': 'created categories tensor column and add values.',\n    'date': '2025-03-03 04:03:13',\n    'info_updated': False,\n    'renamed': OrderedDict(),\n    'deleted': [],\n    'commit_diff_exist': True,\n    'tensor_info_updated': False},\n   {'commit_id': '6536edb5ee077de334d6f23133bcf7e209d1be80',\n    'author': 'public',\n    'message': 'add labels.',\n    'date': '2025-03-03 04:03:12',\n    'info_updated': False,\n    'renamed': OrderedDict(),\n    'deleted': [],\n    'commit_diff_exist': True,\n    'tensor_info_updated': False}],\n  []),\n 'tensor': ([{'commit_id': '51f5bb41c4a1d74d881269366e3718285b8145db',\n    'categories': {'created': True,\n     'cleared': False,\n     'info_updated': False,\n     'data_added': [0, 5],\n     'data_updated': set(),\n     'data_deleted': SortedSet([]),\n     'data_deleted_ids': [],\n     'data_transformed_in_place': False,\n     'add_value': [array(['agent'], dtype='&lt;U5'), array(['emotion'], dtype='&lt;U7'), array(['generation'], dtype='&lt;U10'), array(['writing'], dtype='&lt;U7'), array(['emotion'], dtype='&lt;U7')],\n     'updated_values': [],\n     'data_deleted_values': []},\n    'labels': {'created': False,\n     'cleared': False,\n     'info_updated': False,\n     'data_added': [0, 0],\n     'data_updated': set(),\n     'data_deleted': set(),\n     'data_deleted_ids': [],\n     'data_transformed_in_place': False,\n     'add_value': [],\n     'updated_values': [],\n     'data_deleted_values': []}},\n   {'commit_id': '6536edb5ee077de334d6f23133bcf7e209d1be80',\n    'labels': {'created': False,\n     'cleared': False,\n     'info_updated': False,\n     'data_added': [0, 5],\n     'data_updated': set(),\n     'data_deleted': SortedSet([]),\n     'data_deleted_ids': [],\n     'data_transformed_in_place': False,\n     'add_value': [array([1]), array([2]), array([3]), array([4]), array([5])],\n     'updated_values': [],\n     'data_deleted_values': []}}],\n  [])}\n</code></pre> <ul> <li>For details, see <code>dataset.diff()</code>. For deeper background, see Detailed MR.</li> </ul>"},{"location":"getting_started/4_muller_version_control/#6-what-are-head-changes-how-to-use-reset-to-revert-uncommitted-changes","title":"6. What Are HEAD Changes? How to Use <code>reset()</code> to Revert Uncommitted Changes","text":"<p>Unlike Git, MULLER version control has no local staging area. All changes are immediately synced to the persistent storage location (local or remote). As a result, any dataset change updates the current branch HEAD node immediately. Uncommitted changes do not appear on other branches, but they remain accessible on the current branch until reverted.</p> <p>In the example below, we return to <code>main</code>, which has a <code>labels</code> tensor with 5 samples. We then append one sample and use <code>ds.has_head_changes</code> to confirm there are HEAD changes.</p> <pre><code>&gt;&gt;&gt; ds = muller.dataset(path=\"temp_dataset/\", overwrite=True)\n&gt;&gt;&gt; with ds:\n...     ds.create_tensor(\"labels\", htype=\"generic\")\n...     ds.labels.extend([1, 2, 3, 4, 5])\n&gt;&gt;&gt; ds.commit()\n&gt;&gt;&gt; ds.checkout(\"dev\", create=True)\n&gt;&gt;&gt; ds.checkout(\"main\")\n&gt;&gt;&gt; with ds:\n...     ds.labels.append(100)\n&gt;&gt;&gt; ds.has_head_changes\nTrue\n</code></pre> <p>On the <code>dev</code> branch, the <code>labels</code> tensor still has 5 samples.</p> <pre><code>&gt;&gt;&gt; ds.checkout(\"dev\")\n&gt;&gt;&gt; print(\"Dataset in {} branch has {} samples in the labels tensor\".format(ds.branch, len(ds.labels)))\nDataset in dev branch has 5 samples in the labels tensor\n</code></pre> <p>Switch back to <code>main</code>, and the uncommitted change is still present (6 samples).</p> <pre><code>&gt;&gt;&gt; ds.checkout(\"main\")\n&gt;&gt;&gt; print(\"Dataset in {} branch has {} samples in the labels tensor\".format(ds.branch, len(ds.labels)))\nDataset in main branch has 6 samples in the labels tensor\n&gt;&gt;&gt; ds.has_head_changes\nTrue\n</code></pre> <p>Use <code>ds.reset()</code> to revert uncommitted changes on the current branch.</p> <pre><code>&gt;&gt;&gt; ds.reset()\n&gt;&gt;&gt; ds.has_head_changes\nFalse\n&gt;&gt;&gt; print(\"Dataset in {} branch has {} samples in the labels tensor\".format(ds.branch, len(ds.labels)))\nDataset in main branch has 5 samples in the labels tensor\n</code></pre> <ul> <li>For details, see <code>dataset.reset()</code>.</li> </ul>"},{"location":"getting_started/4_muller_version_control/#7-merge-available-in-v067","title":"7. Merge (available in v0.6.7+)","text":"<p>Branch merging is essential for collaborative workflows. Similar to Git, MULLER supports two merge patterns:</p> <ul> <li>Fast-forward merge (no conflict): if <code>main</code> has not moved since creating the feature branch, merging can fast-forward without creating a new merge commit.   </li> <li>3-way merge (may conflict): if both branches have diverged, MULLER may need to create a new merge commit and may require conflict resolution.   </li> </ul> <p>In multi-branch development, 3-way merges are common. For example: after creating <code>dev_1</code> and <code>dev_2</code> from <code>main</code>, if <code>main</code> merges <code>dev_1</code> first, then <code>main</code> changes and may conflict with <code>dev_2</code>. In other words, after <code>ds.merge(\"dev_1\")</code>, merging <code>dev_2</code> into <code>main</code> can become a 3-way merge.</p> <p></p> <p>MULLER currently follows the merge workflow below:</p> <p></p> <p>1) Use <code>ds.detect_merge_conflict(target_id=\"...\")</code> to preview possible conflicts. High level:</p> <ul> <li>Find the most recent common ancestor (e.g., <code>M1</code>) between the current branch HEAD (e.g., <code>M3</code>) and the target branch head (e.g., <code>D4</code>).</li> <li>Compute diffs <code>D4</code> vs <code>M1</code> and <code>M3</code> vs <code>M1</code>.</li> <li>For tensors common to both branches, compare each sample by its UUID to detect conflicts. If conflicts exist, they are printed and also cached (in-memory) for subsequent merge steps.</li> </ul> <p>Conflict types recorded:</p> <ul> <li>pop conflicts: both sides performed <code>pop</code>, but on different rows (popping the same row does not conflict).</li> <li>update conflicts: both sides updated the same row (updating different rows does not conflict).</li> <li>append conflicts: both sides appended new rows.</li> </ul> <p>2) Merge with <code>ds.merge(target_id=\"...\")</code>. You can choose conflict-resolution strategies:</p> <ul> <li>append_resolution: <code>None</code> (default, raise), <code>\"ours\"</code>, <code>\"theirs\"</code>, <code>\"both\"</code></li> <li>update_resolution: <code>None</code> (default, raise), <code>\"ours\"</code>, <code>\"theirs\"</code></li> <li>pop_resolution: <code>None</code> (default, raise), <code>\"ours\"</code>, <code>\"theirs\"</code>, <code>\"both\"</code></li> </ul> <p>Additional merge parameters:</p> <ul> <li>delete_removed_tensors (bool, default False): if <code>True</code>, tensors deleted in the current-branch diff are discarded.</li> <li>force (bool, default False): relaxes certain rename-related constraints; for example, it may register a renamed tensor as a new tensor if the counterpart tensor is missing on the other side, or merge tensors under certain rename collisions.</li> </ul> <p>Important notes:</p> <ul> <li>Because <code>pop</code> and <code>append</code> change dataset length and global indices, conflict detection/resolution is UUID-based.</li> <li>The execution order of conflict resolutions affects the final result.</li> <li>Be careful to distinguish \u201cconflict\u201d vs \u201cno conflict\u201d. Example: if <code>main</code> has 20 rows, you branch to <code>dev</code>, delete row 2 in <code>dev</code>, then merge <code>dev</code> into <code>main</code>, this conflicts because row 2 still exists in <code>main</code>. You may use <code>pop_resolution=\"theirs\"</code> to accept the deletion from <code>dev</code>.</li> </ul> <p>Example:</p> <pre><code># Create a dataset and add data on main\n&gt;&gt;&gt; ds = muller.dataset(path=\"temp_test\", overwrite=True)\n&gt;&gt;&gt; ds.create_tensor(name=\"labels\", htype=\"generic\", dtype=\"int\")\n&gt;&gt;&gt; ds.labels.extend([0, 1, 2, 3, 4])\n&gt;&gt;&gt; ds.create_tensor(name=\"categories\", htype=\"text\")\n&gt;&gt;&gt; ds.categories.extend([\"a\", \"b\", \"c\", \"d\", \"e\"])\n\n# Create dev-1 and perform add/update/delete operations\n&gt;&gt;&gt; ds.checkout(\"dev-1\", create=True)\n&gt;&gt;&gt; ds.labels.extend([50, 60, 70])\n&gt;&gt;&gt; ds.categories.extend([\"ff\", \"gg\", \"hh\"])\n&gt;&gt;&gt; ds.labels[3] = 30\n&gt;&gt;&gt; ds.pop(1)\n&gt;&gt;&gt; print(ds.labels.numpy())\n[[ 0]\n [ 2]\n [30]\n [ 4]\n [50]\n [60]\n [70]]\n&gt;&gt;&gt; ds.commit()\n\n# Back to main, create dev-2 and perform add/update/delete operations\n&gt;&gt;&gt; ds.checkout(\"main\")\n&gt;&gt;&gt; ds.checkout(\"dev-2\", create=True)\n&gt;&gt;&gt; ds.labels.extend([500, 600, 700, 800])\n&gt;&gt;&gt; ds.categories.extend([\"fff\", \"ggg\", \"hhh\", \"iii\"])\n&gt;&gt;&gt; ds.labels[3] = 300\n&gt;&gt;&gt; ds.labels[4] = 400\n&gt;&gt;&gt; ds.pop([1, 2])\n&gt;&gt;&gt; print(ds.labels.numpy())\n[[  0]\n [300]\n [400]\n [500]\n [600]\n [700]\n [800]]\n&gt;&gt;&gt; ds.commit()\n\n# Merge dev-1 into main (take dev-1's pop changes)\n&gt;&gt;&gt; ds.checkout(\"main\")\n&gt;&gt;&gt; ds.merge(\"dev-1\", pop_resolution=\"theirs\")\n&gt;&gt;&gt; print(ds.labels.numpy())\n[[ 0]\n [ 2]\n [30]\n [ 4]\n [50]\n [60]\n [70]]\n\n# Detect conflicts between main and dev-2\n&gt;&gt;&gt; conflict_tensors, conflict_records = ds.detect_merge_conflict(\"dev-2\", show_value=True)\n&gt;&gt;&gt; pprint(conflict_records)\n{'categories': {'app_ori_idx': [4, 5, 6],\n                'app_ori_values': [array(['ff'], dtype='&lt;U2'),\n                                   array(['gg'], dtype='&lt;U2'),\n                                   array(['hh'], dtype='&lt;U2')],\n                'app_tar_idx': [3, 4, 5, 6],\n                'app_tar_values': [array(['fff'], dtype='&lt;U3'),\n                                   array(['ggg'], dtype='&lt;U3'),\n                                   array(['hhh'], dtype='&lt;U3'),\n                                   array(['iii'], dtype='&lt;U3')],\n                'del_ori_idx': [],\n                'del_ori_values': [],\n                'del_tar_idx': [2],\n                'del_tar_values': [array(['c'], dtype='&lt;U1')],\n                'update_values': {'update_ori': [], 'update_tar': []}},\n 'labels': {'app_ori_idx': [4, 5, 6],\n            'app_ori_values': [array([50]), array([60]), array([70])],\n            'app_tar_idx': [3, 4, 5, 6],\n            'app_tar_values': [array([500]),\n                               array([600]),\n                               array([700]),\n                               array([800])],\n            'del_ori_idx': [],\n            'del_ori_values': [],\n            'del_tar_idx': [2],\n            'del_tar_values': [array([2])],\n            'update_values': {'update_ori': [{2: array([30])}],\n                              'update_tar': [{1: array([300])}]}}}\n\n# Merge dev-2 into main with explicit resolutions\n&gt;&gt;&gt; ds.merge(\"dev-2\", append_resolution=\"both\", pop_resolution=\"ours\", update_resolution=\"theirs\")\n&gt;&gt;&gt; print(ds.labels.numpy())\n[[  0]\n [  2]\n [300]\n [400]\n [ 50]\n [ 60]\n [ 70]\n [500]\n [600]\n [700]\n [800]]\n</code></pre> <ul> <li>For details, see <code>dataset.detect_merge_conflict()</code>.</li> <li>For details, see <code>dataset.merge()</code>.</li> </ul>"},{"location":"getting_started/4_muller_version_control/#8-branch-permission-control-for-the-huashan-platform-available-in-v066","title":"8. Branch Permission Control for the Huashan Platform (available in v0.6.6+)","text":"<p>To meet permission-control requirements on the Huashan platform, the following restrictions apply:</p> <ol> <li>Only the user who created the <code>main</code> branch has write/delete permissions on all branches.</li> <li>Only the <code>main</code> creator can delete or rename a dataset via:</li> </ol> <pre><code>ds.delete()\nds.rename()\n</code></pre> <ol> <li>Non-creators only have write/delete permissions on branches they created, and read-only access on other branches. The following APIs are restricted to \u201cyour own branch\u201d:</li> </ol> <pre><code>ds.reset()\nds.create_tensor()\nds.create_tensor_like()\nds.commit()\nds.extend([dataset_object])\nds.&lt;tensor&gt;.extend()\nds.delete_branch()\nds.rechunk()\nds.append([dataset_object])\nds.&lt;tensor&gt;.append\nds.update()  # there are two update methods\nds.merge()\nds.delete_tensor()\nds.pop()\nds.rename()\nmuller.api_dataset.create_dataset_from_dataframes()\nmuller.api_dataset.create_dataset_from_file()\nds.&lt;tensor&gt;.clear()\nds.create_index()\n</code></pre> <ol> <li>Users can only delete search views they created:</li> </ol> <pre><code>muller.delete_view()\n</code></pre> <ul> <li>Implementation note: in practice, non-creators have write permission to one file on <code>main</code> (<code>version_control_info.json</code>) that stores version metadata.</li> </ul>"},{"location":"getting_started/5_advanced_operations/","title":"5 advanced operations","text":""},{"location":"getting_started/5_advanced_operations/#advanced-operations","title":"Advanced Operations","text":""},{"location":"getting_started/5_advanced_operations/#1-recommendations-for-ingesting-large-scale-data-into-muller-datasets","title":"1. Recommendations for Ingesting Large-Scale Data into MULLER Datasets","text":"<p>If you need to create a dataset or append a large amount of data in the MULLER format, it is recommended to use the <code>@muller.compute</code> decorator for parallel ingestion (typically set <code>num_workers</code> to 8\u201332 depending on available resources).</p> <p>Notes:</p> <ul> <li>Multiprocessing/multithreading has overhead; the speedup becomes noticeable mainly for hundreds of thousands to millions of samples.</li> </ul> <p>Example:</p> <pre><code>def create_cifar10_dataset_parallel(num_workers=4, scheduler=\"threaded\"):\n    ds_multi = muller.dataset(path=\"./temp_test\", overwrite=True)\n    with ds_multi:\n        ds_multi.create_tensor(\"test1\", htype=\"text\")\n        ds_multi.create_tensor(\"test2\", htype=\"text\")\n\n    # Add data row-by-row to preserve row-level atomicity\n    iter_dict = []\n    for i in range(0, 100000):\n        iter_dict.append((i, (\"hi\", \"hello\")))  # Example only; load any data in practice\n\n    @muller.compute\n    def file_to_muller(data_pair, sample_out):\n        sample_out.test1.append(data_pair[1][1])\n        sample_out.test2.append(data_pair[1][0])\n        return sample_out\n\n    with ds_multi:\n        file_to_muller().eval(\n            iter_dict,\n            ds_multi,\n            num_workers=num_workers,\n            scheduler=scheduler,\n            disable_rechunk=True,\n        )\n\n    return ds_multi\n\n\nif __name__ == \"__main__\":\n    ds = create_cifar10_dataset_parallel(num_workers=4, scheduler=\"processed\")\n</code></pre> <p>For large-scale ingestion, <code>eval()</code> also supports <code>checkpoint_interval=&lt;commit_every_N_samples&gt;</code>, which checkpoints to disk every N samples to reduce rework after unexpected interruptions. Internally, data is written before metadata; if a crash occurs mid-write, the system can resume from the previous checkpoint instead of restarting from the first sample.</p> <p>Note: in this case, versions are stored under the <code>/versions</code> directory.</p> <p>With large datasets, not every sample path is guaranteed to be valid (e.g., invalid paths, wrong file formats such as treating PNG as JPEG). You may choose to ignore such errors via <code>.eval(..., ignore_errors=True)</code>; otherwise frequent exception handling can significantly slow down ingestion.</p> <ul> <li>For details, see <code>[muller.compute]()</code>.</li> <li>For details, see <code>[eval()]()</code>.</li> </ul>"},{"location":"getting_started/5_advanced_operations/#2-use-with-for-better-write-performance","title":"2. Use <code>with</code> for Better Write Performance","text":"<ol> <li>In MULLER, each independent update is pushed to the target persistent storage immediately (through an LRU cache; see <code>_set_item()</code> and <code>flush()</code>). If you have many small updates and the data is stored remotely, write time can increase significantly. For example, the following pattern pushes an update on every <code>.append()</code>:</li> </ol> <pre><code>for i in range(10):\n    ds.my_tensor.append(i)\n</code></pre> <ol> <li>Using a <code>with</code> block typically improves performance. Updates are batched and flushed when the <code>with</code> block completes (or when the local cache is full), reducing fragmented writes:</li> </ol> <pre><code>with ds:\n    for i in range(10):\n        ds.my_tensor.append(i)  # or other write operations: create, update, etc.\n</code></pre>"},{"location":"getting_started/5_advanced_operations/#3-why-a-dataset-become-corrupted-and-how-to-recover","title":"3. Why a Dataset Become Corrupted, and How to Recover?","text":"<p>If your program is interrupted unexpectedly (e.g., a crash during append/pop), the dataset may become inconsistent: some tensors may have been updated while others were not. In such cases, you can use <code>ds.reset()</code> to roll back illegal, uncommitted operations and return to the most recent valid commit.</p> <ol> <li>Scenario A: The dataset (or some tensors) cannot be read (e.g., you see an error like below).</li> </ol> <pre><code>DatasetCorruptError: Exception occured (see Traceback). The dataset maybe corrupted. Try using `reset=True` to reset HEAD changes and load the previous commit. This will delete all uncommitted changes on the branch you are trying to load.\n</code></pre> <p>Recovery: reload with <code>reset=True</code>.</p> <pre><code>ds = muller.load(&lt;dataset_path&gt;, reset=True)\n</code></pre> <ol> <li>Scenario B: The dataset is corrupted (e.g., tensor lengths are inconsistent).</li> </ol> <p>Recovery: load without integrity checking, then reset.</p> <pre><code>ds = muller.load(&lt;dataset_path&gt;, check_integrity=False)  # skip integrity check\nds.reset()\n</code></pre> <ul> <li>For details on <code>check_integrity</code> during load, see <code>[muller.dataset()]()</code> and <code>[muller.load()]()</code>.</li> <li>For details on reset, see <code>[dataset.reset()]()</code>.</li> <li>Note: once you reset, all uncommitted changes will be deleted.</li> <li>For large datasets, prefer checkpointing or committing frequently so recovery is easier after unexpected failures.</li> </ul>"},{"location":"getting_started/5_advanced_operations/#4-keys-to-efficient-operations-on-obs","title":"4. Keys to Efficient Operations on OBS","text":"<ol> <li> <p>Use a sufficiently capable OBS client and sufficient bandwidth.</p> </li> <li> <p>The most efficient MULLER usage is typically \u201clocal \u2192 local\u201d or \u201clocal \u2192 OBS bucket\u201d. If you use the Huashan production environment with the <code>huashan://</code> prefix (or an implicit remote path), the call chain may become \u201clocal \u2192 OBS bucket A (personal) \u2192 OBS bucket B (shared)\u201d. Frequent reads/writes via limited OBS APIs between buckets can significantly impact performance.</p> </li> <li>We also expect richer low-level OBS APIs (e.g., batch read/write, batch delete, offset-based partial read/write) to improve small-file transfer efficiency. References: boto3, Huawei OBS.</li> </ol> <p>Note: transferring massive numbers of small files on OBS is inherently costly. Partial read/write can help; see Huawei OBS partial read (with offset).</p> <ol> <li>Use enough memory so the LRU cache can hold more data per flush. If needed, increase <code>DEFAULT_MEMORY_CACHE_SIZE</code> in <code>constants.py</code> (default: 20 GB).</li> </ol> <p>Note: (1) is typically a prerequisite for (2).</p>"},{"location":"getting_started/5_advanced_operations/#5-concurrency-write-locks-in-muller","title":"5. Concurrency: Write Locks in MULLER","text":"<p>MULLER supports basic concurrent-write protection via file locks (including in the Huashan notebook environment). The following locks are used:</p> <ol> <li><code>version_control_info.lock</code>: since branch users can write to <code>version_control_info.json</code>, this lock ensures only one writer at a time; others wait until the lock is released.</li> <li><code>dataset_lock.lock</code>: once a dataset is created in a path, this lock is created. While it exists, creating a new dataset in the same path (e.g., via <code>ds.empty()</code>) is blocked; otherwise you may see:</li> </ol> <pre><code>muller.util.exceptions.DatasetHandlerError: A dataset already exists at the given path (temp_dataset/). If you want to create a new empty dataset, either specify another path or use overwrite=True. If you want to load the dataset that exists at this path, use muller.load() instead.\n</code></pre> <ol> <li><code>queries.lock</code>: used in two cases:</li> <li>Created when <code>ds.save_view()</code> starts and released when it completes, to prevent concurrent use during view saving.</li> <li>Created when <code>ds.delete_view()</code> starts and released when it completes, to prevent concurrent use during deletion. (Currently only the view creator can delete a view, so extreme cases are largely avoided.)</li> </ol> <p>Limitations of file locks:</p> <ul> <li>Lock creation/deletion performance depends on the underlying OBS (NSP) file I/O performance.</li> <li>Lock atomicity depends on the atomicity guarantees of the underlying OBS (NSP) APIs.</li> </ul> <p>MULLER also supports Redis-based distributed locks. Three lock types are currently supported:</p> <ol> <li>Branch-head lock: locks the head-node resources of a branch.</li> <li>Version-control lock: locks shared resources across dataset versions (version-control metadata).</li> <li>Branch lock: locks predecessor versions of an entire branch (not required in v0.6.7 and earlier).</li> </ol> <p>To avoid deadlocks, if you need multiple lock types, acquire them strictly in the order 1 \u2192 2 \u2192 3.</p> <p>Note: MULLER currently has no read locks, so dirty reads are allowed (e.g., user B may observe partial intermediate states while user A is writing).</p>"},{"location":"getting_started/5_advanced_operations/#6-how-is-muller-different-from-deeplake","title":"6. How Is MULLER Different from Deeplake?","text":"<p>Deeplake is closed-source. MULLER adopts a subset of Deeplake-like interfaces but uses its own file layout and includes major performance optimizations and refactoring for version control, loading, and OBS support.</p> <p>Key differences include:</p> <ol> <li>File layout: MULLER\u2019s file organization is self-designed for higher I/O efficiency.</li> <li>Vectorized search acceleration: not available in Deeplake; implemented in MULLER.</li> <li>Version control: Git-for-data style; MULLER provides self-developed <code>merge</code> and <code>diff</code> for more advanced workflows.</li> <li>Multi-user concurrency handling and locks: implemented in MULLER.</li> <li>Branch permission control: implemented in MULLER.</li> <li>High-performance DataLoader: implemented in MULLER.</li> </ol>"},{"location":"getting_started/5_advanced_operations/#7-other","title":"7. Other","text":"<ol> <li>Fetch adjacent data in the chunk</li> </ol> <pre><code># Fetch adjacent data in the chunk -&gt; increases speed when loading sequentially,\n# or when a tensor's data fits in the cache.\nnumeric_label = ds.labels[i].numpy(fetch_chunks=True)\n</code></pre> <p>Note: If <code>True</code>, full chunks will be retrieved from the storage; otherwise only required bytes will be retrieved. This will always be <code>True</code> even if specified as <code>False</code> in the following cases: (1) the tensor is ChunkCompressed; (2) the chunk being accessed has more than 128 samples.</p>"},{"location":"getting_started/installation/","title":"Install","text":""},{"location":"getting_started/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python &gt;= 3.11</li> <li>CMake &gt;= 3.22.1 (required for building C++ extensions)</li> <li>A C++17 compatible compiler (tested with gcc 11.4.0)</li> <li>Linux or macOS (tested on Ubuntu 22.04)</li> </ul>"},{"location":"getting_started/installation/#1-recommended-create-a-new-conda-environment","title":"1. (Recommended) Create a new Conda environment","text":"<pre><code>conda create -n muller python=3.11\nconda activate muller\n</code></pre>"},{"location":"getting_started/installation/#2-installation","title":"2. Installation","text":"<ul> <li>First, clone the MULLER repository. <pre><code>git clone https://github.com/The-AI-Framework-and-Data-Tech-Lab-HK/MULLER.git\ncd MULLER\nchmod 777 muller/util/sparsehash/build_proj.sh  # You may need to modify the script permissions.\n</code></pre></li> <li>[Dafault] Install from code <pre><code>pip install .   # Use `pip install . -v` to view detailed build logs\n</code></pre></li> <li>[Optional] Development (editable) installation <pre><code>pip install -e .\n</code></pre></li> <li>[Optional] Skip building C++ modules</li> </ul> <p>The Python implementation provides the same core functionality as the C++ modules. If you only need the basic features of MULLER, you may skip building the C++ extensions: <pre><code>BUILD_CPP=false pip install .\n</code></pre></p>"},{"location":"getting_started/installation/#3-verify-the-installation","title":"3. Verify the Installation","text":"<pre><code>import muller\nprint(muller.__version__)\n</code></pre>"}]}